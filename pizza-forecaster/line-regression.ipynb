{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-21T15:31:35.603107Z",
     "start_time": "2025-05-21T15:31:35.473036Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea\n",
    "\n",
    "\n",
    "def predict(X, w, b):\n",
    "    return X * w + b\n",
    "\n",
    "\n",
    "def loss(X, Y, w, b):\n",
    "    return np.average((predict(X, w, b) - Y) ** 2)\n",
    "\n",
    "\n",
    "def train(X, Y, iterations, learning_rate):\n",
    "    w = b = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X, Y, w, b)\n",
    "        print(\"Iteration %4d => Loss: %.6f\" % (i, current_loss))\n",
    "\n",
    "        if loss(X, Y, w + learning_rate, b) < current_loss:\n",
    "            w += learning_rate\n",
    "        elif loss(X, Y, w - learning_rate, b) < current_loss:\n",
    "            w -= learning_rate\n",
    "        elif loss(X, Y, w, b + learning_rate) < current_loss:\n",
    "            b += learning_rate\n",
    "        elif loss(X, Y, w, b - learning_rate) < current_loss:\n",
    "            b -= learning_rate\n",
    "        else:\n",
    "            return w, b\n",
    "\n",
    "    raise Exception(\"Couldn't converge within %d iterations\" % iterations)\n",
    "\n",
    "\n",
    "reservations, pizzas = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)\n",
    "\n",
    "# Train the system\n",
    "w, b = train(reservations, pizzas, 10000, 0.01)\n",
    "print(\"\\nw=%.3f, b=%.3f\" % (w, b))\n",
    "\n",
    "# Predict the number of pizzas\n",
    "print(\"Prediction: x=%d => y=%.2f\" % (20, predict(20, w, b)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 812.866667\n",
      "Iteration    1 => Loss: 804.820547\n",
      "Iteration    2 => Loss: 796.818187\n",
      "Iteration    3 => Loss: 788.859587\n",
      "Iteration    4 => Loss: 780.944747\n",
      "Iteration    5 => Loss: 773.073667\n",
      "Iteration    6 => Loss: 765.246347\n",
      "Iteration    7 => Loss: 757.462787\n",
      "Iteration    8 => Loss: 749.722987\n",
      "Iteration    9 => Loss: 742.026947\n",
      "Iteration   10 => Loss: 734.374667\n",
      "Iteration   11 => Loss: 726.766147\n",
      "Iteration   12 => Loss: 719.201387\n",
      "Iteration   13 => Loss: 711.680387\n",
      "Iteration   14 => Loss: 704.203147\n",
      "Iteration   15 => Loss: 696.769667\n",
      "Iteration   16 => Loss: 689.379947\n",
      "Iteration   17 => Loss: 682.033987\n",
      "Iteration   18 => Loss: 674.731787\n",
      "Iteration   19 => Loss: 667.473347\n",
      "Iteration   20 => Loss: 660.258667\n",
      "Iteration   21 => Loss: 653.087747\n",
      "Iteration   22 => Loss: 645.960587\n",
      "Iteration   23 => Loss: 638.877187\n",
      "Iteration   24 => Loss: 631.837547\n",
      "Iteration   25 => Loss: 624.841667\n",
      "Iteration   26 => Loss: 617.889547\n",
      "Iteration   27 => Loss: 610.981187\n",
      "Iteration   28 => Loss: 604.116587\n",
      "Iteration   29 => Loss: 597.295747\n",
      "Iteration   30 => Loss: 590.518667\n",
      "Iteration   31 => Loss: 583.785347\n",
      "Iteration   32 => Loss: 577.095787\n",
      "Iteration   33 => Loss: 570.449987\n",
      "Iteration   34 => Loss: 563.847947\n",
      "Iteration   35 => Loss: 557.289667\n",
      "Iteration   36 => Loss: 550.775147\n",
      "Iteration   37 => Loss: 544.304387\n",
      "Iteration   38 => Loss: 537.877387\n",
      "Iteration   39 => Loss: 531.494147\n",
      "Iteration   40 => Loss: 525.154667\n",
      "Iteration   41 => Loss: 518.858947\n",
      "Iteration   42 => Loss: 512.606987\n",
      "Iteration   43 => Loss: 506.398787\n",
      "Iteration   44 => Loss: 500.234347\n",
      "Iteration   45 => Loss: 494.113667\n",
      "Iteration   46 => Loss: 488.036747\n",
      "Iteration   47 => Loss: 482.003587\n",
      "Iteration   48 => Loss: 476.014187\n",
      "Iteration   49 => Loss: 470.068547\n",
      "Iteration   50 => Loss: 464.166667\n",
      "Iteration   51 => Loss: 458.308547\n",
      "Iteration   52 => Loss: 452.494187\n",
      "Iteration   53 => Loss: 446.723587\n",
      "Iteration   54 => Loss: 440.996747\n",
      "Iteration   55 => Loss: 435.313667\n",
      "Iteration   56 => Loss: 429.674347\n",
      "Iteration   57 => Loss: 424.078787\n",
      "Iteration   58 => Loss: 418.526987\n",
      "Iteration   59 => Loss: 413.018947\n",
      "Iteration   60 => Loss: 407.554667\n",
      "Iteration   61 => Loss: 402.134147\n",
      "Iteration   62 => Loss: 396.757387\n",
      "Iteration   63 => Loss: 391.424387\n",
      "Iteration   64 => Loss: 386.135147\n",
      "Iteration   65 => Loss: 380.889667\n",
      "Iteration   66 => Loss: 375.687947\n",
      "Iteration   67 => Loss: 370.529987\n",
      "Iteration   68 => Loss: 365.415787\n",
      "Iteration   69 => Loss: 360.345347\n",
      "Iteration   70 => Loss: 355.318667\n",
      "Iteration   71 => Loss: 350.335747\n",
      "Iteration   72 => Loss: 345.396587\n",
      "Iteration   73 => Loss: 340.501187\n",
      "Iteration   74 => Loss: 335.649547\n",
      "Iteration   75 => Loss: 330.841667\n",
      "Iteration   76 => Loss: 326.077547\n",
      "Iteration   77 => Loss: 321.357187\n",
      "Iteration   78 => Loss: 316.680587\n",
      "Iteration   79 => Loss: 312.047747\n",
      "Iteration   80 => Loss: 307.458667\n",
      "Iteration   81 => Loss: 302.913347\n",
      "Iteration   82 => Loss: 298.411787\n",
      "Iteration   83 => Loss: 293.953987\n",
      "Iteration   84 => Loss: 289.539947\n",
      "Iteration   85 => Loss: 285.169667\n",
      "Iteration   86 => Loss: 280.843147\n",
      "Iteration   87 => Loss: 276.560387\n",
      "Iteration   88 => Loss: 272.321387\n",
      "Iteration   89 => Loss: 268.126147\n",
      "Iteration   90 => Loss: 263.974667\n",
      "Iteration   91 => Loss: 259.866947\n",
      "Iteration   92 => Loss: 255.802987\n",
      "Iteration   93 => Loss: 251.782787\n",
      "Iteration   94 => Loss: 247.806347\n",
      "Iteration   95 => Loss: 243.873667\n",
      "Iteration   96 => Loss: 239.984747\n",
      "Iteration   97 => Loss: 236.139587\n",
      "Iteration   98 => Loss: 232.338187\n",
      "Iteration   99 => Loss: 228.580547\n",
      "Iteration  100 => Loss: 224.866667\n",
      "Iteration  101 => Loss: 221.196547\n",
      "Iteration  102 => Loss: 217.570187\n",
      "Iteration  103 => Loss: 213.987587\n",
      "Iteration  104 => Loss: 210.448747\n",
      "Iteration  105 => Loss: 206.953667\n",
      "Iteration  106 => Loss: 203.502347\n",
      "Iteration  107 => Loss: 200.094787\n",
      "Iteration  108 => Loss: 196.730987\n",
      "Iteration  109 => Loss: 193.410947\n",
      "Iteration  110 => Loss: 190.134667\n",
      "Iteration  111 => Loss: 186.902147\n",
      "Iteration  112 => Loss: 183.713387\n",
      "Iteration  113 => Loss: 180.568387\n",
      "Iteration  114 => Loss: 177.467147\n",
      "Iteration  115 => Loss: 174.409667\n",
      "Iteration  116 => Loss: 171.395947\n",
      "Iteration  117 => Loss: 168.425987\n",
      "Iteration  118 => Loss: 165.499787\n",
      "Iteration  119 => Loss: 162.617347\n",
      "Iteration  120 => Loss: 159.778667\n",
      "Iteration  121 => Loss: 156.983747\n",
      "Iteration  122 => Loss: 154.232587\n",
      "Iteration  123 => Loss: 151.525187\n",
      "Iteration  124 => Loss: 148.861547\n",
      "Iteration  125 => Loss: 146.241667\n",
      "Iteration  126 => Loss: 143.665547\n",
      "Iteration  127 => Loss: 141.133187\n",
      "Iteration  128 => Loss: 138.644587\n",
      "Iteration  129 => Loss: 136.199747\n",
      "Iteration  130 => Loss: 133.798667\n",
      "Iteration  131 => Loss: 131.441347\n",
      "Iteration  132 => Loss: 129.127787\n",
      "Iteration  133 => Loss: 126.857987\n",
      "Iteration  134 => Loss: 124.631947\n",
      "Iteration  135 => Loss: 122.449667\n",
      "Iteration  136 => Loss: 120.311147\n",
      "Iteration  137 => Loss: 118.216387\n",
      "Iteration  138 => Loss: 116.165387\n",
      "Iteration  139 => Loss: 114.158147\n",
      "Iteration  140 => Loss: 112.194667\n",
      "Iteration  141 => Loss: 110.274947\n",
      "Iteration  142 => Loss: 108.398987\n",
      "Iteration  143 => Loss: 106.566787\n",
      "Iteration  144 => Loss: 104.778347\n",
      "Iteration  145 => Loss: 103.033667\n",
      "Iteration  146 => Loss: 101.332747\n",
      "Iteration  147 => Loss: 99.675587\n",
      "Iteration  148 => Loss: 98.062187\n",
      "Iteration  149 => Loss: 96.492547\n",
      "Iteration  150 => Loss: 94.966667\n",
      "Iteration  151 => Loss: 93.484547\n",
      "Iteration  152 => Loss: 92.046187\n",
      "Iteration  153 => Loss: 90.651587\n",
      "Iteration  154 => Loss: 89.300747\n",
      "Iteration  155 => Loss: 87.993667\n",
      "Iteration  156 => Loss: 86.730347\n",
      "Iteration  157 => Loss: 85.510787\n",
      "Iteration  158 => Loss: 84.334987\n",
      "Iteration  159 => Loss: 83.202947\n",
      "Iteration  160 => Loss: 82.114667\n",
      "Iteration  161 => Loss: 81.070147\n",
      "Iteration  162 => Loss: 80.069387\n",
      "Iteration  163 => Loss: 79.112387\n",
      "Iteration  164 => Loss: 78.199147\n",
      "Iteration  165 => Loss: 77.329667\n",
      "Iteration  166 => Loss: 76.503947\n",
      "Iteration  167 => Loss: 75.721987\n",
      "Iteration  168 => Loss: 74.983787\n",
      "Iteration  169 => Loss: 74.289347\n",
      "Iteration  170 => Loss: 73.638667\n",
      "Iteration  171 => Loss: 73.031747\n",
      "Iteration  172 => Loss: 72.468587\n",
      "Iteration  173 => Loss: 71.949187\n",
      "Iteration  174 => Loss: 71.473547\n",
      "Iteration  175 => Loss: 71.041667\n",
      "Iteration  176 => Loss: 70.653547\n",
      "Iteration  177 => Loss: 70.309187\n",
      "Iteration  178 => Loss: 70.008587\n",
      "Iteration  179 => Loss: 69.751747\n",
      "Iteration  180 => Loss: 69.538667\n",
      "Iteration  181 => Loss: 69.369347\n",
      "Iteration  182 => Loss: 69.243787\n",
      "Iteration  183 => Loss: 69.161987\n",
      "Iteration  184 => Loss: 69.123947\n",
      "Iteration  185 => Loss: 69.052847\n",
      "Iteration  186 => Loss: 68.981947\n",
      "Iteration  187 => Loss: 68.911247\n",
      "Iteration  188 => Loss: 68.840747\n",
      "Iteration  189 => Loss: 68.770447\n",
      "Iteration  190 => Loss: 68.700347\n",
      "Iteration  191 => Loss: 68.630447\n",
      "Iteration  192 => Loss: 68.560747\n",
      "Iteration  193 => Loss: 68.491247\n",
      "Iteration  194 => Loss: 68.421947\n",
      "Iteration  195 => Loss: 68.352847\n",
      "Iteration  196 => Loss: 68.283947\n",
      "Iteration  197 => Loss: 68.215247\n",
      "Iteration  198 => Loss: 68.146747\n",
      "Iteration  199 => Loss: 68.078447\n",
      "Iteration  200 => Loss: 68.010347\n",
      "Iteration  201 => Loss: 68.007853\n",
      "Iteration  202 => Loss: 67.937420\n",
      "Iteration  203 => Loss: 67.867187\n",
      "Iteration  204 => Loss: 67.797153\n",
      "Iteration  205 => Loss: 67.727320\n",
      "Iteration  206 => Loss: 67.657687\n",
      "Iteration  207 => Loss: 67.588253\n",
      "Iteration  208 => Loss: 67.519020\n",
      "Iteration  209 => Loss: 67.449987\n",
      "Iteration  210 => Loss: 67.381153\n",
      "Iteration  211 => Loss: 67.312520\n",
      "Iteration  212 => Loss: 67.244087\n",
      "Iteration  213 => Loss: 67.175853\n",
      "Iteration  214 => Loss: 67.107820\n",
      "Iteration  215 => Loss: 67.039987\n",
      "Iteration  216 => Loss: 66.972353\n",
      "Iteration  217 => Loss: 66.904920\n",
      "Iteration  218 => Loss: 66.837687\n",
      "Iteration  219 => Loss: 66.835887\n",
      "Iteration  220 => Loss: 66.766320\n",
      "Iteration  221 => Loss: 66.696953\n",
      "Iteration  222 => Loss: 66.627787\n",
      "Iteration  223 => Loss: 66.558820\n",
      "Iteration  224 => Loss: 66.490053\n",
      "Iteration  225 => Loss: 66.421487\n",
      "Iteration  226 => Loss: 66.353120\n",
      "Iteration  227 => Loss: 66.284953\n",
      "Iteration  228 => Loss: 66.216987\n",
      "Iteration  229 => Loss: 66.149220\n",
      "Iteration  230 => Loss: 66.081653\n",
      "Iteration  231 => Loss: 66.014287\n",
      "Iteration  232 => Loss: 65.947120\n",
      "Iteration  233 => Loss: 65.880153\n",
      "Iteration  234 => Loss: 65.813387\n",
      "Iteration  235 => Loss: 65.746820\n",
      "Iteration  236 => Loss: 65.680453\n",
      "Iteration  237 => Loss: 65.679347\n",
      "Iteration  238 => Loss: 65.610647\n",
      "Iteration  239 => Loss: 65.542147\n",
      "Iteration  240 => Loss: 65.473847\n",
      "Iteration  241 => Loss: 65.405747\n",
      "Iteration  242 => Loss: 65.337847\n",
      "Iteration  243 => Loss: 65.270147\n",
      "Iteration  244 => Loss: 65.202647\n",
      "Iteration  245 => Loss: 65.135347\n",
      "Iteration  246 => Loss: 65.068247\n",
      "Iteration  247 => Loss: 65.001347\n",
      "Iteration  248 => Loss: 64.934647\n",
      "Iteration  249 => Loss: 64.868147\n",
      "Iteration  250 => Loss: 64.801847\n",
      "Iteration  251 => Loss: 64.735747\n",
      "Iteration  252 => Loss: 64.669847\n",
      "Iteration  253 => Loss: 64.604147\n",
      "Iteration  254 => Loss: 64.538647\n",
      "Iteration  255 => Loss: 64.538233\n",
      "Iteration  256 => Loss: 64.470400\n",
      "Iteration  257 => Loss: 64.402767\n",
      "Iteration  258 => Loss: 64.335333\n",
      "Iteration  259 => Loss: 64.268100\n",
      "Iteration  260 => Loss: 64.201067\n",
      "Iteration  261 => Loss: 64.134233\n",
      "Iteration  262 => Loss: 64.067600\n",
      "Iteration  263 => Loss: 64.001167\n",
      "Iteration  264 => Loss: 63.934933\n",
      "Iteration  265 => Loss: 63.868900\n",
      "Iteration  266 => Loss: 63.803067\n",
      "Iteration  267 => Loss: 63.737433\n",
      "Iteration  268 => Loss: 63.672000\n",
      "Iteration  269 => Loss: 63.606767\n",
      "Iteration  270 => Loss: 63.541733\n",
      "Iteration  271 => Loss: 63.476900\n",
      "Iteration  272 => Loss: 63.412267\n",
      "Iteration  273 => Loss: 63.347833\n",
      "Iteration  274 => Loss: 63.345580\n",
      "Iteration  275 => Loss: 63.278813\n",
      "Iteration  276 => Loss: 63.212247\n",
      "Iteration  277 => Loss: 63.145880\n",
      "Iteration  278 => Loss: 63.079713\n",
      "Iteration  279 => Loss: 63.013747\n",
      "Iteration  280 => Loss: 62.947980\n",
      "Iteration  281 => Loss: 62.882413\n",
      "Iteration  282 => Loss: 62.817047\n",
      "Iteration  283 => Loss: 62.751880\n",
      "Iteration  284 => Loss: 62.686913\n",
      "Iteration  285 => Loss: 62.622147\n",
      "Iteration  286 => Loss: 62.557580\n",
      "Iteration  287 => Loss: 62.493213\n",
      "Iteration  288 => Loss: 62.429047\n",
      "Iteration  289 => Loss: 62.365080\n",
      "Iteration  290 => Loss: 62.301313\n",
      "Iteration  291 => Loss: 62.237747\n",
      "Iteration  292 => Loss: 62.236187\n",
      "Iteration  293 => Loss: 62.170287\n",
      "Iteration  294 => Loss: 62.104587\n",
      "Iteration  295 => Loss: 62.039087\n",
      "Iteration  296 => Loss: 61.973787\n",
      "Iteration  297 => Loss: 61.908687\n",
      "Iteration  298 => Loss: 61.843787\n",
      "Iteration  299 => Loss: 61.779087\n",
      "Iteration  300 => Loss: 61.714587\n",
      "Iteration  301 => Loss: 61.650287\n",
      "Iteration  302 => Loss: 61.586187\n",
      "Iteration  303 => Loss: 61.522287\n",
      "Iteration  304 => Loss: 61.458587\n",
      "Iteration  305 => Loss: 61.395087\n",
      "Iteration  306 => Loss: 61.331787\n",
      "Iteration  307 => Loss: 61.268687\n",
      "Iteration  308 => Loss: 61.205787\n",
      "Iteration  309 => Loss: 61.143087\n",
      "Iteration  310 => Loss: 61.142220\n",
      "Iteration  311 => Loss: 61.077187\n",
      "Iteration  312 => Loss: 61.012353\n",
      "Iteration  313 => Loss: 60.947720\n",
      "Iteration  314 => Loss: 60.883287\n",
      "Iteration  315 => Loss: 60.819053\n",
      "Iteration  316 => Loss: 60.755020\n",
      "Iteration  317 => Loss: 60.691187\n",
      "Iteration  318 => Loss: 60.627553\n",
      "Iteration  319 => Loss: 60.564120\n",
      "Iteration  320 => Loss: 60.500887\n",
      "Iteration  321 => Loss: 60.437853\n",
      "Iteration  322 => Loss: 60.375020\n",
      "Iteration  323 => Loss: 60.312387\n",
      "Iteration  324 => Loss: 60.249953\n",
      "Iteration  325 => Loss: 60.187720\n",
      "Iteration  326 => Loss: 60.125687\n",
      "Iteration  327 => Loss: 60.063853\n",
      "Iteration  328 => Loss: 60.063680\n",
      "Iteration  329 => Loss: 59.999513\n",
      "Iteration  330 => Loss: 59.935547\n",
      "Iteration  331 => Loss: 59.871780\n",
      "Iteration  332 => Loss: 59.808213\n",
      "Iteration  333 => Loss: 59.744847\n",
      "Iteration  334 => Loss: 59.681680\n",
      "Iteration  335 => Loss: 59.618713\n",
      "Iteration  336 => Loss: 59.555947\n",
      "Iteration  337 => Loss: 59.493380\n",
      "Iteration  338 => Loss: 59.431013\n",
      "Iteration  339 => Loss: 59.368847\n",
      "Iteration  340 => Loss: 59.306880\n",
      "Iteration  341 => Loss: 59.245113\n",
      "Iteration  342 => Loss: 59.183547\n",
      "Iteration  343 => Loss: 59.122180\n",
      "Iteration  344 => Loss: 59.061013\n",
      "Iteration  345 => Loss: 59.000047\n",
      "Iteration  346 => Loss: 58.939280\n",
      "Iteration  347 => Loss: 58.937267\n",
      "Iteration  348 => Loss: 58.874167\n",
      "Iteration  349 => Loss: 58.811267\n",
      "Iteration  350 => Loss: 58.748567\n",
      "Iteration  351 => Loss: 58.686067\n",
      "Iteration  352 => Loss: 58.623767\n",
      "Iteration  353 => Loss: 58.561667\n",
      "Iteration  354 => Loss: 58.499767\n",
      "Iteration  355 => Loss: 58.438067\n",
      "Iteration  356 => Loss: 58.376567\n",
      "Iteration  357 => Loss: 58.315267\n",
      "Iteration  358 => Loss: 58.254167\n",
      "Iteration  359 => Loss: 58.193267\n",
      "Iteration  360 => Loss: 58.132567\n",
      "Iteration  361 => Loss: 58.072067\n",
      "Iteration  362 => Loss: 58.011767\n",
      "Iteration  363 => Loss: 57.951667\n",
      "Iteration  364 => Loss: 57.891767\n",
      "Iteration  365 => Loss: 57.890447\n",
      "Iteration  366 => Loss: 57.828213\n",
      "Iteration  367 => Loss: 57.766180\n",
      "Iteration  368 => Loss: 57.704347\n",
      "Iteration  369 => Loss: 57.642713\n",
      "Iteration  370 => Loss: 57.581280\n",
      "Iteration  371 => Loss: 57.520047\n",
      "Iteration  372 => Loss: 57.459013\n",
      "Iteration  373 => Loss: 57.398180\n",
      "Iteration  374 => Loss: 57.337547\n",
      "Iteration  375 => Loss: 57.277113\n",
      "Iteration  376 => Loss: 57.216880\n",
      "Iteration  377 => Loss: 57.156847\n",
      "Iteration  378 => Loss: 57.097013\n",
      "Iteration  379 => Loss: 57.037380\n",
      "Iteration  380 => Loss: 56.977947\n",
      "Iteration  381 => Loss: 56.918713\n",
      "Iteration  382 => Loss: 56.859680\n",
      "Iteration  383 => Loss: 56.859053\n",
      "Iteration  384 => Loss: 56.797687\n",
      "Iteration  385 => Loss: 56.736520\n",
      "Iteration  386 => Loss: 56.675553\n",
      "Iteration  387 => Loss: 56.614787\n",
      "Iteration  388 => Loss: 56.554220\n",
      "Iteration  389 => Loss: 56.493853\n",
      "Iteration  390 => Loss: 56.433687\n",
      "Iteration  391 => Loss: 56.373720\n",
      "Iteration  392 => Loss: 56.313953\n",
      "Iteration  393 => Loss: 56.254387\n",
      "Iteration  394 => Loss: 56.195020\n",
      "Iteration  395 => Loss: 56.135853\n",
      "Iteration  396 => Loss: 56.076887\n",
      "Iteration  397 => Loss: 56.018120\n",
      "Iteration  398 => Loss: 55.959553\n",
      "Iteration  399 => Loss: 55.901187\n",
      "Iteration  400 => Loss: 55.843020\n",
      "Iteration  401 => Loss: 55.785053\n",
      "Iteration  402 => Loss: 55.782587\n",
      "Iteration  403 => Loss: 55.722287\n",
      "Iteration  404 => Loss: 55.662187\n",
      "Iteration  405 => Loss: 55.602287\n",
      "Iteration  406 => Loss: 55.542587\n",
      "Iteration  407 => Loss: 55.483087\n",
      "Iteration  408 => Loss: 55.423787\n",
      "Iteration  409 => Loss: 55.364687\n",
      "Iteration  410 => Loss: 55.305787\n",
      "Iteration  411 => Loss: 55.247087\n",
      "Iteration  412 => Loss: 55.188587\n",
      "Iteration  413 => Loss: 55.130287\n",
      "Iteration  414 => Loss: 55.072187\n",
      "Iteration  415 => Loss: 55.014287\n",
      "Iteration  416 => Loss: 54.956587\n",
      "Iteration  417 => Loss: 54.899087\n",
      "Iteration  418 => Loss: 54.841787\n",
      "Iteration  419 => Loss: 54.784687\n",
      "Iteration  420 => Loss: 54.782913\n",
      "Iteration  421 => Loss: 54.723480\n",
      "Iteration  422 => Loss: 54.664247\n",
      "Iteration  423 => Loss: 54.605213\n",
      "Iteration  424 => Loss: 54.546380\n",
      "Iteration  425 => Loss: 54.487747\n",
      "Iteration  426 => Loss: 54.429313\n",
      "Iteration  427 => Loss: 54.371080\n",
      "Iteration  428 => Loss: 54.313047\n",
      "Iteration  429 => Loss: 54.255213\n",
      "Iteration  430 => Loss: 54.197580\n",
      "Iteration  431 => Loss: 54.140147\n",
      "Iteration  432 => Loss: 54.082913\n",
      "Iteration  433 => Loss: 54.025880\n",
      "Iteration  434 => Loss: 53.969047\n",
      "Iteration  435 => Loss: 53.912413\n",
      "Iteration  436 => Loss: 53.855980\n",
      "Iteration  437 => Loss: 53.799747\n",
      "Iteration  438 => Loss: 53.798667\n",
      "Iteration  439 => Loss: 53.740100\n",
      "Iteration  440 => Loss: 53.681733\n",
      "Iteration  441 => Loss: 53.623567\n",
      "Iteration  442 => Loss: 53.565600\n",
      "Iteration  443 => Loss: 53.507833\n",
      "Iteration  444 => Loss: 53.450267\n",
      "Iteration  445 => Loss: 53.392900\n",
      "Iteration  446 => Loss: 53.335733\n",
      "Iteration  447 => Loss: 53.278767\n",
      "Iteration  448 => Loss: 53.222000\n",
      "Iteration  449 => Loss: 53.165433\n",
      "Iteration  450 => Loss: 53.109067\n",
      "Iteration  451 => Loss: 53.052900\n",
      "Iteration  452 => Loss: 52.996933\n",
      "Iteration  453 => Loss: 52.941167\n",
      "Iteration  454 => Loss: 52.885600\n",
      "Iteration  455 => Loss: 52.830233\n",
      "Iteration  456 => Loss: 52.829847\n",
      "Iteration  457 => Loss: 52.772147\n",
      "Iteration  458 => Loss: 52.714647\n",
      "Iteration  459 => Loss: 52.657347\n",
      "Iteration  460 => Loss: 52.600247\n",
      "Iteration  461 => Loss: 52.543347\n",
      "Iteration  462 => Loss: 52.486647\n",
      "Iteration  463 => Loss: 52.430147\n",
      "Iteration  464 => Loss: 52.373847\n",
      "Iteration  465 => Loss: 52.317747\n",
      "Iteration  466 => Loss: 52.261847\n",
      "Iteration  467 => Loss: 52.206147\n",
      "Iteration  468 => Loss: 52.150647\n",
      "Iteration  469 => Loss: 52.095347\n",
      "Iteration  470 => Loss: 52.040247\n",
      "Iteration  471 => Loss: 51.985347\n",
      "Iteration  472 => Loss: 51.930647\n",
      "Iteration  473 => Loss: 51.876147\n",
      "Iteration  474 => Loss: 51.821847\n",
      "Iteration  475 => Loss: 51.819620\n",
      "Iteration  476 => Loss: 51.762987\n",
      "Iteration  477 => Loss: 51.706553\n",
      "Iteration  478 => Loss: 51.650320\n",
      "Iteration  479 => Loss: 51.594287\n",
      "Iteration  480 => Loss: 51.538453\n",
      "Iteration  481 => Loss: 51.482820\n",
      "Iteration  482 => Loss: 51.427387\n",
      "Iteration  483 => Loss: 51.372153\n",
      "Iteration  484 => Loss: 51.317120\n",
      "Iteration  485 => Loss: 51.262287\n",
      "Iteration  486 => Loss: 51.207653\n",
      "Iteration  487 => Loss: 51.153220\n",
      "Iteration  488 => Loss: 51.098987\n",
      "Iteration  489 => Loss: 51.044953\n",
      "Iteration  490 => Loss: 50.991120\n",
      "Iteration  491 => Loss: 50.937487\n",
      "Iteration  492 => Loss: 50.884053\n",
      "Iteration  493 => Loss: 50.882520\n",
      "Iteration  494 => Loss: 50.826753\n",
      "Iteration  495 => Loss: 50.771187\n",
      "Iteration  496 => Loss: 50.715820\n",
      "Iteration  497 => Loss: 50.660653\n",
      "Iteration  498 => Loss: 50.605687\n",
      "Iteration  499 => Loss: 50.550920\n",
      "Iteration  500 => Loss: 50.496353\n",
      "Iteration  501 => Loss: 50.441987\n",
      "Iteration  502 => Loss: 50.387820\n",
      "Iteration  503 => Loss: 50.333853\n",
      "Iteration  504 => Loss: 50.280087\n",
      "Iteration  505 => Loss: 50.226520\n",
      "Iteration  506 => Loss: 50.173153\n",
      "Iteration  507 => Loss: 50.119987\n",
      "Iteration  508 => Loss: 50.067020\n",
      "Iteration  509 => Loss: 50.014253\n",
      "Iteration  510 => Loss: 49.961687\n",
      "Iteration  511 => Loss: 49.960847\n",
      "Iteration  512 => Loss: 49.905947\n",
      "Iteration  513 => Loss: 49.851247\n",
      "Iteration  514 => Loss: 49.796747\n",
      "Iteration  515 => Loss: 49.742447\n",
      "Iteration  516 => Loss: 49.688347\n",
      "Iteration  517 => Loss: 49.634447\n",
      "Iteration  518 => Loss: 49.580747\n",
      "Iteration  519 => Loss: 49.527247\n",
      "Iteration  520 => Loss: 49.473947\n",
      "Iteration  521 => Loss: 49.420847\n",
      "Iteration  522 => Loss: 49.367947\n",
      "Iteration  523 => Loss: 49.315247\n",
      "Iteration  524 => Loss: 49.262747\n",
      "Iteration  525 => Loss: 49.210447\n",
      "Iteration  526 => Loss: 49.158347\n",
      "Iteration  527 => Loss: 49.106447\n",
      "Iteration  528 => Loss: 49.054747\n",
      "Iteration  529 => Loss: 49.054600\n",
      "Iteration  530 => Loss: 49.000567\n",
      "Iteration  531 => Loss: 48.946733\n",
      "Iteration  532 => Loss: 48.893100\n",
      "Iteration  533 => Loss: 48.839667\n",
      "Iteration  534 => Loss: 48.786433\n",
      "Iteration  535 => Loss: 48.733400\n",
      "Iteration  536 => Loss: 48.680567\n",
      "Iteration  537 => Loss: 48.627933\n",
      "Iteration  538 => Loss: 48.575500\n",
      "Iteration  539 => Loss: 48.523267\n",
      "Iteration  540 => Loss: 48.471233\n",
      "Iteration  541 => Loss: 48.419400\n",
      "Iteration  542 => Loss: 48.367767\n",
      "Iteration  543 => Loss: 48.316333\n",
      "Iteration  544 => Loss: 48.265100\n",
      "Iteration  545 => Loss: 48.214067\n",
      "Iteration  546 => Loss: 48.163233\n",
      "Iteration  547 => Loss: 48.112600\n",
      "Iteration  548 => Loss: 48.110613\n",
      "Iteration  549 => Loss: 48.057647\n",
      "Iteration  550 => Loss: 48.004880\n",
      "Iteration  551 => Loss: 47.952313\n",
      "Iteration  552 => Loss: 47.899947\n",
      "Iteration  553 => Loss: 47.847780\n",
      "Iteration  554 => Loss: 47.795813\n",
      "Iteration  555 => Loss: 47.744047\n",
      "Iteration  556 => Loss: 47.692480\n",
      "Iteration  557 => Loss: 47.641113\n",
      "Iteration  558 => Loss: 47.589947\n",
      "Iteration  559 => Loss: 47.538980\n",
      "Iteration  560 => Loss: 47.488213\n",
      "Iteration  561 => Loss: 47.437647\n",
      "Iteration  562 => Loss: 47.387280\n",
      "Iteration  563 => Loss: 47.337113\n",
      "Iteration  564 => Loss: 47.287147\n",
      "Iteration  565 => Loss: 47.237380\n",
      "Iteration  566 => Loss: 47.236087\n",
      "Iteration  567 => Loss: 47.183987\n",
      "Iteration  568 => Loss: 47.132087\n",
      "Iteration  569 => Loss: 47.080387\n",
      "Iteration  570 => Loss: 47.028887\n",
      "Iteration  571 => Loss: 46.977587\n",
      "Iteration  572 => Loss: 46.926487\n",
      "Iteration  573 => Loss: 46.875587\n",
      "Iteration  574 => Loss: 46.824887\n",
      "Iteration  575 => Loss: 46.774387\n",
      "Iteration  576 => Loss: 46.724087\n",
      "Iteration  577 => Loss: 46.673987\n",
      "Iteration  578 => Loss: 46.624087\n",
      "Iteration  579 => Loss: 46.574387\n",
      "Iteration  580 => Loss: 46.524887\n",
      "Iteration  581 => Loss: 46.475587\n",
      "Iteration  582 => Loss: 46.426487\n",
      "Iteration  583 => Loss: 46.377587\n",
      "Iteration  584 => Loss: 46.376987\n",
      "Iteration  585 => Loss: 46.325753\n",
      "Iteration  586 => Loss: 46.274720\n",
      "Iteration  587 => Loss: 46.223887\n",
      "Iteration  588 => Loss: 46.173253\n",
      "Iteration  589 => Loss: 46.122820\n",
      "Iteration  590 => Loss: 46.072587\n",
      "Iteration  591 => Loss: 46.022553\n",
      "Iteration  592 => Loss: 45.972720\n",
      "Iteration  593 => Loss: 45.923087\n",
      "Iteration  594 => Loss: 45.873653\n",
      "Iteration  595 => Loss: 45.824420\n",
      "Iteration  596 => Loss: 45.775387\n",
      "Iteration  597 => Loss: 45.726553\n",
      "Iteration  598 => Loss: 45.677920\n",
      "Iteration  599 => Loss: 45.629487\n",
      "Iteration  600 => Loss: 45.581253\n",
      "Iteration  601 => Loss: 45.533220\n",
      "Iteration  602 => Loss: 45.485387\n",
      "Iteration  603 => Loss: 45.482947\n",
      "Iteration  604 => Loss: 45.432780\n",
      "Iteration  605 => Loss: 45.382813\n",
      "Iteration  606 => Loss: 45.333047\n",
      "Iteration  607 => Loss: 45.283480\n",
      "Iteration  608 => Loss: 45.234113\n",
      "Iteration  609 => Loss: 45.184947\n",
      "Iteration  610 => Loss: 45.135980\n",
      "Iteration  611 => Loss: 45.087213\n",
      "Iteration  612 => Loss: 45.038647\n",
      "Iteration  613 => Loss: 44.990280\n",
      "Iteration  614 => Loss: 44.942113\n",
      "Iteration  615 => Loss: 44.894147\n",
      "Iteration  616 => Loss: 44.846380\n",
      "Iteration  617 => Loss: 44.798813\n",
      "Iteration  618 => Loss: 44.751447\n",
      "Iteration  619 => Loss: 44.704280\n",
      "Iteration  620 => Loss: 44.657313\n",
      "Iteration  621 => Loss: 44.655567\n",
      "Iteration  622 => Loss: 44.606267\n",
      "Iteration  623 => Loss: 44.557167\n",
      "Iteration  624 => Loss: 44.508267\n",
      "Iteration  625 => Loss: 44.459567\n",
      "Iteration  626 => Loss: 44.411067\n",
      "Iteration  627 => Loss: 44.362767\n",
      "Iteration  628 => Loss: 44.314667\n",
      "Iteration  629 => Loss: 44.266767\n",
      "Iteration  630 => Loss: 44.219067\n",
      "Iteration  631 => Loss: 44.171567\n",
      "Iteration  632 => Loss: 44.124267\n",
      "Iteration  633 => Loss: 44.077167\n",
      "Iteration  634 => Loss: 44.030267\n",
      "Iteration  635 => Loss: 43.983567\n",
      "Iteration  636 => Loss: 43.937067\n",
      "Iteration  637 => Loss: 43.890767\n",
      "Iteration  638 => Loss: 43.844667\n",
      "Iteration  639 => Loss: 43.843613\n",
      "Iteration  640 => Loss: 43.795180\n",
      "Iteration  641 => Loss: 43.746947\n",
      "Iteration  642 => Loss: 43.698913\n",
      "Iteration  643 => Loss: 43.651080\n",
      "Iteration  644 => Loss: 43.603447\n",
      "Iteration  645 => Loss: 43.556013\n",
      "Iteration  646 => Loss: 43.508780\n",
      "Iteration  647 => Loss: 43.461747\n",
      "Iteration  648 => Loss: 43.414913\n",
      "Iteration  649 => Loss: 43.368280\n",
      "Iteration  650 => Loss: 43.321847\n",
      "Iteration  651 => Loss: 43.275613\n",
      "Iteration  652 => Loss: 43.229580\n",
      "Iteration  653 => Loss: 43.183747\n",
      "Iteration  654 => Loss: 43.138113\n",
      "Iteration  655 => Loss: 43.092680\n",
      "Iteration  656 => Loss: 43.047447\n",
      "Iteration  657 => Loss: 43.047087\n",
      "Iteration  658 => Loss: 42.999520\n",
      "Iteration  659 => Loss: 42.952153\n",
      "Iteration  660 => Loss: 42.904987\n",
      "Iteration  661 => Loss: 42.858020\n",
      "Iteration  662 => Loss: 42.811253\n",
      "Iteration  663 => Loss: 42.764687\n",
      "Iteration  664 => Loss: 42.718320\n",
      "Iteration  665 => Loss: 42.672153\n",
      "Iteration  666 => Loss: 42.626187\n",
      "Iteration  667 => Loss: 42.580420\n",
      "Iteration  668 => Loss: 42.534853\n",
      "Iteration  669 => Loss: 42.489487\n",
      "Iteration  670 => Loss: 42.444320\n",
      "Iteration  671 => Loss: 42.399353\n",
      "Iteration  672 => Loss: 42.354587\n",
      "Iteration  673 => Loss: 42.310020\n",
      "Iteration  674 => Loss: 42.265653\n",
      "Iteration  675 => Loss: 42.221487\n",
      "Iteration  676 => Loss: 42.219287\n",
      "Iteration  677 => Loss: 42.172787\n",
      "Iteration  678 => Loss: 42.126487\n",
      "Iteration  679 => Loss: 42.080387\n",
      "Iteration  680 => Loss: 42.034487\n",
      "Iteration  681 => Loss: 41.988787\n",
      "Iteration  682 => Loss: 41.943287\n",
      "Iteration  683 => Loss: 41.897987\n",
      "Iteration  684 => Loss: 41.852887\n",
      "Iteration  685 => Loss: 41.807987\n",
      "Iteration  686 => Loss: 41.763287\n",
      "Iteration  687 => Loss: 41.718787\n",
      "Iteration  688 => Loss: 41.674487\n",
      "Iteration  689 => Loss: 41.630387\n",
      "Iteration  690 => Loss: 41.586487\n",
      "Iteration  691 => Loss: 41.542787\n",
      "Iteration  692 => Loss: 41.499287\n",
      "Iteration  693 => Loss: 41.455987\n",
      "Iteration  694 => Loss: 41.454480\n",
      "Iteration  695 => Loss: 41.408847\n",
      "Iteration  696 => Loss: 41.363413\n",
      "Iteration  697 => Loss: 41.318180\n",
      "Iteration  698 => Loss: 41.273147\n",
      "Iteration  699 => Loss: 41.228313\n",
      "Iteration  700 => Loss: 41.183680\n",
      "Iteration  701 => Loss: 41.139247\n",
      "Iteration  702 => Loss: 41.095013\n",
      "Iteration  703 => Loss: 41.050980\n",
      "Iteration  704 => Loss: 41.007147\n",
      "Iteration  705 => Loss: 40.963513\n",
      "Iteration  706 => Loss: 40.920080\n",
      "Iteration  707 => Loss: 40.876847\n",
      "Iteration  708 => Loss: 40.833813\n",
      "Iteration  709 => Loss: 40.790980\n",
      "Iteration  710 => Loss: 40.748347\n",
      "Iteration  711 => Loss: 40.705913\n",
      "Iteration  712 => Loss: 40.705100\n",
      "Iteration  713 => Loss: 40.660333\n",
      "Iteration  714 => Loss: 40.615767\n",
      "Iteration  715 => Loss: 40.571400\n",
      "Iteration  716 => Loss: 40.527233\n",
      "Iteration  717 => Loss: 40.483267\n",
      "Iteration  718 => Loss: 40.439500\n",
      "Iteration  719 => Loss: 40.395933\n",
      "Iteration  720 => Loss: 40.352567\n",
      "Iteration  721 => Loss: 40.309400\n",
      "Iteration  722 => Loss: 40.266433\n",
      "Iteration  723 => Loss: 40.223667\n",
      "Iteration  724 => Loss: 40.181100\n",
      "Iteration  725 => Loss: 40.138733\n",
      "Iteration  726 => Loss: 40.096567\n",
      "Iteration  727 => Loss: 40.054600\n",
      "Iteration  728 => Loss: 40.012833\n",
      "Iteration  729 => Loss: 39.971267\n",
      "Iteration  730 => Loss: 39.971147\n",
      "Iteration  731 => Loss: 39.927247\n",
      "Iteration  732 => Loss: 39.883547\n",
      "Iteration  733 => Loss: 39.840047\n",
      "Iteration  734 => Loss: 39.796747\n",
      "Iteration  735 => Loss: 39.753647\n",
      "Iteration  736 => Loss: 39.710747\n",
      "Iteration  737 => Loss: 39.668047\n",
      "Iteration  738 => Loss: 39.625547\n",
      "Iteration  739 => Loss: 39.583247\n",
      "Iteration  740 => Loss: 39.541147\n",
      "Iteration  741 => Loss: 39.499247\n",
      "Iteration  742 => Loss: 39.457547\n",
      "Iteration  743 => Loss: 39.416047\n",
      "Iteration  744 => Loss: 39.374747\n",
      "Iteration  745 => Loss: 39.333647\n",
      "Iteration  746 => Loss: 39.292747\n",
      "Iteration  747 => Loss: 39.252047\n",
      "Iteration  748 => Loss: 39.211547\n",
      "Iteration  749 => Loss: 39.209587\n",
      "Iteration  750 => Loss: 39.166753\n",
      "Iteration  751 => Loss: 39.124120\n",
      "Iteration  752 => Loss: 39.081687\n",
      "Iteration  753 => Loss: 39.039453\n",
      "Iteration  754 => Loss: 38.997420\n",
      "Iteration  755 => Loss: 38.955587\n",
      "Iteration  756 => Loss: 38.913953\n",
      "Iteration  757 => Loss: 38.872520\n",
      "Iteration  758 => Loss: 38.831287\n",
      "Iteration  759 => Loss: 38.790253\n",
      "Iteration  760 => Loss: 38.749420\n",
      "Iteration  761 => Loss: 38.708787\n",
      "Iteration  762 => Loss: 38.668353\n",
      "Iteration  763 => Loss: 38.628120\n",
      "Iteration  764 => Loss: 38.588087\n",
      "Iteration  765 => Loss: 38.548253\n",
      "Iteration  766 => Loss: 38.508620\n",
      "Iteration  767 => Loss: 38.507353\n",
      "Iteration  768 => Loss: 38.465387\n",
      "Iteration  769 => Loss: 38.423620\n",
      "Iteration  770 => Loss: 38.382053\n",
      "Iteration  771 => Loss: 38.340687\n",
      "Iteration  772 => Loss: 38.299520\n",
      "Iteration  773 => Loss: 38.258553\n",
      "Iteration  774 => Loss: 38.217787\n",
      "Iteration  775 => Loss: 38.177220\n",
      "Iteration  776 => Loss: 38.136853\n",
      "Iteration  777 => Loss: 38.096687\n",
      "Iteration  778 => Loss: 38.056720\n",
      "Iteration  779 => Loss: 38.016953\n",
      "Iteration  780 => Loss: 37.977387\n",
      "Iteration  781 => Loss: 37.938020\n",
      "Iteration  782 => Loss: 37.898853\n",
      "Iteration  783 => Loss: 37.859887\n",
      "Iteration  784 => Loss: 37.821120\n",
      "Iteration  785 => Loss: 37.820547\n",
      "Iteration  786 => Loss: 37.779447\n",
      "Iteration  787 => Loss: 37.738547\n",
      "Iteration  788 => Loss: 37.697847\n",
      "Iteration  789 => Loss: 37.657347\n",
      "Iteration  790 => Loss: 37.617047\n",
      "Iteration  791 => Loss: 37.576947\n",
      "Iteration  792 => Loss: 37.537047\n",
      "Iteration  793 => Loss: 37.497347\n",
      "Iteration  794 => Loss: 37.457847\n",
      "Iteration  795 => Loss: 37.418547\n",
      "Iteration  796 => Loss: 37.379447\n",
      "Iteration  797 => Loss: 37.340547\n",
      "Iteration  798 => Loss: 37.301847\n",
      "Iteration  799 => Loss: 37.263347\n",
      "Iteration  800 => Loss: 37.225047\n",
      "Iteration  801 => Loss: 37.186947\n",
      "Iteration  802 => Loss: 37.149047\n",
      "Iteration  803 => Loss: 37.111347\n",
      "Iteration  804 => Loss: 37.108933\n",
      "Iteration  805 => Loss: 37.068900\n",
      "Iteration  806 => Loss: 37.029067\n",
      "Iteration  807 => Loss: 36.989433\n",
      "Iteration  808 => Loss: 36.950000\n",
      "Iteration  809 => Loss: 36.910767\n",
      "Iteration  810 => Loss: 36.871733\n",
      "Iteration  811 => Loss: 36.832900\n",
      "Iteration  812 => Loss: 36.794267\n",
      "Iteration  813 => Loss: 36.755833\n",
      "Iteration  814 => Loss: 36.717600\n",
      "Iteration  815 => Loss: 36.679567\n",
      "Iteration  816 => Loss: 36.641733\n",
      "Iteration  817 => Loss: 36.604100\n",
      "Iteration  818 => Loss: 36.566667\n",
      "Iteration  819 => Loss: 36.529433\n",
      "Iteration  820 => Loss: 36.492400\n",
      "Iteration  821 => Loss: 36.455567\n",
      "Iteration  822 => Loss: 36.453847\n",
      "Iteration  823 => Loss: 36.414680\n",
      "Iteration  824 => Loss: 36.375713\n",
      "Iteration  825 => Loss: 36.336947\n",
      "Iteration  826 => Loss: 36.298380\n",
      "Iteration  827 => Loss: 36.260013\n",
      "Iteration  828 => Loss: 36.221847\n",
      "Iteration  829 => Loss: 36.183880\n",
      "Iteration  830 => Loss: 36.146113\n",
      "Iteration  831 => Loss: 36.108547\n",
      "Iteration  832 => Loss: 36.071180\n",
      "Iteration  833 => Loss: 36.034013\n",
      "Iteration  834 => Loss: 35.997047\n",
      "Iteration  835 => Loss: 35.960280\n",
      "Iteration  836 => Loss: 35.923713\n",
      "Iteration  837 => Loss: 35.887347\n",
      "Iteration  838 => Loss: 35.851180\n",
      "Iteration  839 => Loss: 35.815213\n",
      "Iteration  840 => Loss: 35.814187\n",
      "Iteration  841 => Loss: 35.775887\n",
      "Iteration  842 => Loss: 35.737787\n",
      "Iteration  843 => Loss: 35.699887\n",
      "Iteration  844 => Loss: 35.662187\n",
      "Iteration  845 => Loss: 35.624687\n",
      "Iteration  846 => Loss: 35.587387\n",
      "Iteration  847 => Loss: 35.550287\n",
      "Iteration  848 => Loss: 35.513387\n",
      "Iteration  849 => Loss: 35.476687\n",
      "Iteration  850 => Loss: 35.440187\n",
      "Iteration  851 => Loss: 35.403887\n",
      "Iteration  852 => Loss: 35.367787\n",
      "Iteration  853 => Loss: 35.331887\n",
      "Iteration  854 => Loss: 35.296187\n",
      "Iteration  855 => Loss: 35.260687\n",
      "Iteration  856 => Loss: 35.225387\n",
      "Iteration  857 => Loss: 35.190287\n",
      "Iteration  858 => Loss: 35.189953\n",
      "Iteration  859 => Loss: 35.152520\n",
      "Iteration  860 => Loss: 35.115287\n",
      "Iteration  861 => Loss: 35.078253\n",
      "Iteration  862 => Loss: 35.041420\n",
      "Iteration  863 => Loss: 35.004787\n",
      "Iteration  864 => Loss: 34.968353\n",
      "Iteration  865 => Loss: 34.932120\n",
      "Iteration  866 => Loss: 34.896087\n",
      "Iteration  867 => Loss: 34.860253\n",
      "Iteration  868 => Loss: 34.824620\n",
      "Iteration  869 => Loss: 34.789187\n",
      "Iteration  870 => Loss: 34.753953\n",
      "Iteration  871 => Loss: 34.718920\n",
      "Iteration  872 => Loss: 34.684087\n",
      "Iteration  873 => Loss: 34.649453\n",
      "Iteration  874 => Loss: 34.615020\n",
      "Iteration  875 => Loss: 34.580787\n",
      "Iteration  876 => Loss: 34.546753\n",
      "Iteration  877 => Loss: 34.544580\n",
      "Iteration  878 => Loss: 34.508213\n",
      "Iteration  879 => Loss: 34.472047\n",
      "Iteration  880 => Loss: 34.436080\n",
      "Iteration  881 => Loss: 34.400313\n",
      "Iteration  882 => Loss: 34.364747\n",
      "Iteration  883 => Loss: 34.329380\n",
      "Iteration  884 => Loss: 34.294213\n",
      "Iteration  885 => Loss: 34.259247\n",
      "Iteration  886 => Loss: 34.224480\n",
      "Iteration  887 => Loss: 34.189913\n",
      "Iteration  888 => Loss: 34.155547\n",
      "Iteration  889 => Loss: 34.121380\n",
      "Iteration  890 => Loss: 34.087413\n",
      "Iteration  891 => Loss: 34.053647\n",
      "Iteration  892 => Loss: 34.020080\n",
      "Iteration  893 => Loss: 33.986713\n",
      "Iteration  894 => Loss: 33.953547\n",
      "Iteration  895 => Loss: 33.952067\n",
      "Iteration  896 => Loss: 33.916567\n",
      "Iteration  897 => Loss: 33.881267\n",
      "Iteration  898 => Loss: 33.846167\n",
      "Iteration  899 => Loss: 33.811267\n",
      "Iteration  900 => Loss: 33.776567\n",
      "Iteration  901 => Loss: 33.742067\n",
      "Iteration  902 => Loss: 33.707767\n",
      "Iteration  903 => Loss: 33.673667\n",
      "Iteration  904 => Loss: 33.639767\n",
      "Iteration  905 => Loss: 33.606067\n",
      "Iteration  906 => Loss: 33.572567\n",
      "Iteration  907 => Loss: 33.539267\n",
      "Iteration  908 => Loss: 33.506167\n",
      "Iteration  909 => Loss: 33.473267\n",
      "Iteration  910 => Loss: 33.440567\n",
      "Iteration  911 => Loss: 33.408067\n",
      "Iteration  912 => Loss: 33.375767\n",
      "Iteration  913 => Loss: 33.374980\n",
      "Iteration  914 => Loss: 33.340347\n",
      "Iteration  915 => Loss: 33.305913\n",
      "Iteration  916 => Loss: 33.271680\n",
      "Iteration  917 => Loss: 33.237647\n",
      "Iteration  918 => Loss: 33.203813\n",
      "Iteration  919 => Loss: 33.170180\n",
      "Iteration  920 => Loss: 33.136747\n",
      "Iteration  921 => Loss: 33.103513\n",
      "Iteration  922 => Loss: 33.070480\n",
      "Iteration  923 => Loss: 33.037647\n",
      "Iteration  924 => Loss: 33.005013\n",
      "Iteration  925 => Loss: 32.972580\n",
      "Iteration  926 => Loss: 32.940347\n",
      "Iteration  927 => Loss: 32.908313\n",
      "Iteration  928 => Loss: 32.876480\n",
      "Iteration  929 => Loss: 32.844847\n",
      "Iteration  930 => Loss: 32.813413\n",
      "Iteration  931 => Loss: 32.813320\n",
      "Iteration  932 => Loss: 32.779553\n",
      "Iteration  933 => Loss: 32.745987\n",
      "Iteration  934 => Loss: 32.712620\n",
      "Iteration  935 => Loss: 32.679453\n",
      "Iteration  936 => Loss: 32.646487\n",
      "Iteration  937 => Loss: 32.613720\n",
      "Iteration  938 => Loss: 32.581153\n",
      "Iteration  939 => Loss: 32.548787\n",
      "Iteration  940 => Loss: 32.516620\n",
      "Iteration  941 => Loss: 32.484653\n",
      "Iteration  942 => Loss: 32.452887\n",
      "Iteration  943 => Loss: 32.421320\n",
      "Iteration  944 => Loss: 32.389953\n",
      "Iteration  945 => Loss: 32.358787\n",
      "Iteration  946 => Loss: 32.327820\n",
      "Iteration  947 => Loss: 32.297053\n",
      "Iteration  948 => Loss: 32.266487\n",
      "Iteration  949 => Loss: 32.236120\n",
      "Iteration  950 => Loss: 32.234187\n",
      "Iteration  951 => Loss: 32.201487\n",
      "Iteration  952 => Loss: 32.168987\n",
      "Iteration  953 => Loss: 32.136687\n",
      "Iteration  954 => Loss: 32.104587\n",
      "Iteration  955 => Loss: 32.072687\n",
      "Iteration  956 => Loss: 32.040987\n",
      "Iteration  957 => Loss: 32.009487\n",
      "Iteration  958 => Loss: 31.978187\n",
      "Iteration  959 => Loss: 31.947087\n",
      "Iteration  960 => Loss: 31.916187\n",
      "Iteration  961 => Loss: 31.885487\n",
      "Iteration  962 => Loss: 31.854987\n",
      "Iteration  963 => Loss: 31.824687\n",
      "Iteration  964 => Loss: 31.794587\n",
      "Iteration  965 => Loss: 31.764687\n",
      "Iteration  966 => Loss: 31.734987\n",
      "Iteration  967 => Loss: 31.705487\n",
      "Iteration  968 => Loss: 31.704247\n",
      "Iteration  969 => Loss: 31.672413\n",
      "Iteration  970 => Loss: 31.640780\n",
      "Iteration  971 => Loss: 31.609347\n",
      "Iteration  972 => Loss: 31.578113\n",
      "Iteration  973 => Loss: 31.547080\n",
      "Iteration  974 => Loss: 31.516247\n",
      "Iteration  975 => Loss: 31.485613\n",
      "Iteration  976 => Loss: 31.455180\n",
      "Iteration  977 => Loss: 31.424947\n",
      "Iteration  978 => Loss: 31.394913\n",
      "Iteration  979 => Loss: 31.365080\n",
      "Iteration  980 => Loss: 31.335447\n",
      "Iteration  981 => Loss: 31.306013\n",
      "Iteration  982 => Loss: 31.276780\n",
      "Iteration  983 => Loss: 31.247747\n",
      "Iteration  984 => Loss: 31.218913\n",
      "Iteration  985 => Loss: 31.190280\n",
      "Iteration  986 => Loss: 31.189733\n",
      "Iteration  987 => Loss: 31.158767\n",
      "Iteration  988 => Loss: 31.128000\n",
      "Iteration  989 => Loss: 31.097433\n",
      "Iteration  990 => Loss: 31.067067\n",
      "Iteration  991 => Loss: 31.036900\n",
      "Iteration  992 => Loss: 31.006933\n",
      "Iteration  993 => Loss: 30.977167\n",
      "Iteration  994 => Loss: 30.947600\n",
      "Iteration  995 => Loss: 30.918233\n",
      "Iteration  996 => Loss: 30.889067\n",
      "Iteration  997 => Loss: 30.860100\n",
      "Iteration  998 => Loss: 30.831333\n",
      "Iteration  999 => Loss: 30.802767\n",
      "Iteration 1000 => Loss: 30.774400\n",
      "Iteration 1001 => Loss: 30.746233\n",
      "Iteration 1002 => Loss: 30.718267\n",
      "Iteration 1003 => Loss: 30.690500\n",
      "Iteration 1004 => Loss: 30.662933\n",
      "Iteration 1005 => Loss: 30.660547\n",
      "Iteration 1006 => Loss: 30.630647\n",
      "Iteration 1007 => Loss: 30.600947\n",
      "Iteration 1008 => Loss: 30.571447\n",
      "Iteration 1009 => Loss: 30.542147\n",
      "Iteration 1010 => Loss: 30.513047\n",
      "Iteration 1011 => Loss: 30.484147\n",
      "Iteration 1012 => Loss: 30.455447\n",
      "Iteration 1013 => Loss: 30.426947\n",
      "Iteration 1014 => Loss: 30.398647\n",
      "Iteration 1015 => Loss: 30.370547\n",
      "Iteration 1016 => Loss: 30.342647\n",
      "Iteration 1017 => Loss: 30.314947\n",
      "Iteration 1018 => Loss: 30.287447\n",
      "Iteration 1019 => Loss: 30.260147\n",
      "Iteration 1020 => Loss: 30.233047\n",
      "Iteration 1021 => Loss: 30.206147\n",
      "Iteration 1022 => Loss: 30.179447\n",
      "Iteration 1023 => Loss: 30.177753\n",
      "Iteration 1024 => Loss: 30.148720\n",
      "Iteration 1025 => Loss: 30.119887\n",
      "Iteration 1026 => Loss: 30.091253\n",
      "Iteration 1027 => Loss: 30.062820\n",
      "Iteration 1028 => Loss: 30.034587\n",
      "Iteration 1029 => Loss: 30.006553\n",
      "Iteration 1030 => Loss: 29.978720\n",
      "Iteration 1031 => Loss: 29.951087\n",
      "Iteration 1032 => Loss: 29.923653\n",
      "Iteration 1033 => Loss: 29.896420\n",
      "Iteration 1034 => Loss: 29.869387\n",
      "Iteration 1035 => Loss: 29.842553\n",
      "Iteration 1036 => Loss: 29.815920\n",
      "Iteration 1037 => Loss: 29.789487\n",
      "Iteration 1038 => Loss: 29.763253\n",
      "Iteration 1039 => Loss: 29.737220\n",
      "Iteration 1040 => Loss: 29.711387\n",
      "Iteration 1041 => Loss: 29.710387\n",
      "Iteration 1042 => Loss: 29.682220\n",
      "Iteration 1043 => Loss: 29.654253\n",
      "Iteration 1044 => Loss: 29.626487\n",
      "Iteration 1045 => Loss: 29.598920\n",
      "Iteration 1046 => Loss: 29.571553\n",
      "Iteration 1047 => Loss: 29.544387\n",
      "Iteration 1048 => Loss: 29.517420\n",
      "Iteration 1049 => Loss: 29.490653\n",
      "Iteration 1050 => Loss: 29.464087\n",
      "Iteration 1051 => Loss: 29.437720\n",
      "Iteration 1052 => Loss: 29.411553\n",
      "Iteration 1053 => Loss: 29.385587\n",
      "Iteration 1054 => Loss: 29.359820\n",
      "Iteration 1055 => Loss: 29.334253\n",
      "Iteration 1056 => Loss: 29.308887\n",
      "Iteration 1057 => Loss: 29.283720\n",
      "Iteration 1058 => Loss: 29.258753\n",
      "Iteration 1059 => Loss: 29.258447\n",
      "Iteration 1060 => Loss: 29.231147\n",
      "Iteration 1061 => Loss: 29.204047\n",
      "Iteration 1062 => Loss: 29.177147\n",
      "Iteration 1063 => Loss: 29.150447\n",
      "Iteration 1064 => Loss: 29.123947\n",
      "Iteration 1065 => Loss: 29.097647\n",
      "Iteration 1066 => Loss: 29.071547\n",
      "Iteration 1067 => Loss: 29.045647\n",
      "Iteration 1068 => Loss: 29.019947\n",
      "Iteration 1069 => Loss: 28.994447\n",
      "Iteration 1070 => Loss: 28.969147\n",
      "Iteration 1071 => Loss: 28.944047\n",
      "Iteration 1072 => Loss: 28.919147\n",
      "Iteration 1073 => Loss: 28.894447\n",
      "Iteration 1074 => Loss: 28.869947\n",
      "Iteration 1075 => Loss: 28.845647\n",
      "Iteration 1076 => Loss: 28.821547\n",
      "Iteration 1077 => Loss: 28.797647\n",
      "Iteration 1078 => Loss: 28.795500\n",
      "Iteration 1079 => Loss: 28.769267\n",
      "Iteration 1080 => Loss: 28.743233\n",
      "Iteration 1081 => Loss: 28.717400\n",
      "Iteration 1082 => Loss: 28.691767\n",
      "Iteration 1083 => Loss: 28.666333\n",
      "Iteration 1084 => Loss: 28.641100\n",
      "Iteration 1085 => Loss: 28.616067\n",
      "Iteration 1086 => Loss: 28.591233\n",
      "Iteration 1087 => Loss: 28.566600\n",
      "Iteration 1088 => Loss: 28.542167\n",
      "Iteration 1089 => Loss: 28.517933\n",
      "Iteration 1090 => Loss: 28.493900\n",
      "Iteration 1091 => Loss: 28.470067\n",
      "Iteration 1092 => Loss: 28.446433\n",
      "Iteration 1093 => Loss: 28.423000\n",
      "Iteration 1094 => Loss: 28.399767\n",
      "Iteration 1095 => Loss: 28.376733\n",
      "Iteration 1096 => Loss: 28.375280\n",
      "Iteration 1097 => Loss: 28.349913\n",
      "Iteration 1098 => Loss: 28.324747\n",
      "Iteration 1099 => Loss: 28.299780\n",
      "Iteration 1100 => Loss: 28.275013\n",
      "Iteration 1101 => Loss: 28.250447\n",
      "Iteration 1102 => Loss: 28.226080\n",
      "Iteration 1103 => Loss: 28.201913\n",
      "Iteration 1104 => Loss: 28.177947\n",
      "Iteration 1105 => Loss: 28.154180\n",
      "Iteration 1106 => Loss: 28.130613\n",
      "Iteration 1107 => Loss: 28.107247\n",
      "Iteration 1108 => Loss: 28.084080\n",
      "Iteration 1109 => Loss: 28.061113\n",
      "Iteration 1110 => Loss: 28.038347\n",
      "Iteration 1111 => Loss: 28.015780\n",
      "Iteration 1112 => Loss: 27.993413\n",
      "Iteration 1113 => Loss: 27.971247\n",
      "Iteration 1114 => Loss: 27.970487\n",
      "Iteration 1115 => Loss: 27.945987\n",
      "Iteration 1116 => Loss: 27.921687\n",
      "Iteration 1117 => Loss: 27.897587\n",
      "Iteration 1118 => Loss: 27.873687\n",
      "Iteration 1119 => Loss: 27.849987\n",
      "Iteration 1120 => Loss: 27.826487\n",
      "Iteration 1121 => Loss: 27.803187\n",
      "Iteration 1122 => Loss: 27.780087\n",
      "Iteration 1123 => Loss: 27.757187\n",
      "Iteration 1124 => Loss: 27.734487\n",
      "Iteration 1125 => Loss: 27.711987\n",
      "Iteration 1126 => Loss: 27.689687\n",
      "Iteration 1127 => Loss: 27.667587\n",
      "Iteration 1128 => Loss: 27.645687\n",
      "Iteration 1129 => Loss: 27.623987\n",
      "Iteration 1130 => Loss: 27.602487\n",
      "Iteration 1131 => Loss: 27.581187\n",
      "Iteration 1132 => Loss: 27.581120\n",
      "Iteration 1133 => Loss: 27.557487\n",
      "Iteration 1134 => Loss: 27.534053\n",
      "Iteration 1135 => Loss: 27.510820\n",
      "Iteration 1136 => Loss: 27.487787\n",
      "Iteration 1137 => Loss: 27.464953\n",
      "Iteration 1138 => Loss: 27.442320\n",
      "Iteration 1139 => Loss: 27.419887\n",
      "Iteration 1140 => Loss: 27.397653\n",
      "Iteration 1141 => Loss: 27.375620\n",
      "Iteration 1142 => Loss: 27.353787\n",
      "Iteration 1143 => Loss: 27.332153\n",
      "Iteration 1144 => Loss: 27.310720\n",
      "Iteration 1145 => Loss: 27.289487\n",
      "Iteration 1146 => Loss: 27.268453\n",
      "Iteration 1147 => Loss: 27.247620\n",
      "Iteration 1148 => Loss: 27.226987\n",
      "Iteration 1149 => Loss: 27.206553\n",
      "Iteration 1150 => Loss: 27.186320\n",
      "Iteration 1151 => Loss: 27.184413\n",
      "Iteration 1152 => Loss: 27.161847\n",
      "Iteration 1153 => Loss: 27.139480\n",
      "Iteration 1154 => Loss: 27.117313\n",
      "Iteration 1155 => Loss: 27.095347\n",
      "Iteration 1156 => Loss: 27.073580\n",
      "Iteration 1157 => Loss: 27.052013\n",
      "Iteration 1158 => Loss: 27.030647\n",
      "Iteration 1159 => Loss: 27.009480\n",
      "Iteration 1160 => Loss: 26.988513\n",
      "Iteration 1161 => Loss: 26.967747\n",
      "Iteration 1162 => Loss: 26.947180\n",
      "Iteration 1163 => Loss: 26.926813\n",
      "Iteration 1164 => Loss: 26.906647\n",
      "Iteration 1165 => Loss: 26.886680\n",
      "Iteration 1166 => Loss: 26.866913\n",
      "Iteration 1167 => Loss: 26.847347\n",
      "Iteration 1168 => Loss: 26.827980\n",
      "Iteration 1169 => Loss: 26.826767\n",
      "Iteration 1170 => Loss: 26.805067\n",
      "Iteration 1171 => Loss: 26.783567\n",
      "Iteration 1172 => Loss: 26.762267\n",
      "Iteration 1173 => Loss: 26.741167\n",
      "Iteration 1174 => Loss: 26.720267\n",
      "Iteration 1175 => Loss: 26.699567\n",
      "Iteration 1176 => Loss: 26.679067\n",
      "Iteration 1177 => Loss: 26.658767\n",
      "Iteration 1178 => Loss: 26.638667\n",
      "Iteration 1179 => Loss: 26.618767\n",
      "Iteration 1180 => Loss: 26.599067\n",
      "Iteration 1181 => Loss: 26.579567\n",
      "Iteration 1182 => Loss: 26.560267\n",
      "Iteration 1183 => Loss: 26.541167\n",
      "Iteration 1184 => Loss: 26.522267\n",
      "Iteration 1185 => Loss: 26.503567\n",
      "Iteration 1186 => Loss: 26.485067\n",
      "Iteration 1187 => Loss: 26.484547\n",
      "Iteration 1188 => Loss: 26.463713\n",
      "Iteration 1189 => Loss: 26.443080\n",
      "Iteration 1190 => Loss: 26.422647\n",
      "Iteration 1191 => Loss: 26.402413\n",
      "Iteration 1192 => Loss: 26.382380\n",
      "Iteration 1193 => Loss: 26.362547\n",
      "Iteration 1194 => Loss: 26.342913\n",
      "Iteration 1195 => Loss: 26.323480\n",
      "Iteration 1196 => Loss: 26.304247\n",
      "Iteration 1197 => Loss: 26.285213\n",
      "Iteration 1198 => Loss: 26.266380\n",
      "Iteration 1199 => Loss: 26.247747\n",
      "Iteration 1200 => Loss: 26.229313\n",
      "Iteration 1201 => Loss: 26.211080\n",
      "Iteration 1202 => Loss: 26.193047\n",
      "Iteration 1203 => Loss: 26.175213\n",
      "Iteration 1204 => Loss: 26.157580\n",
      "Iteration 1205 => Loss: 26.140147\n",
      "Iteration 1206 => Loss: 26.137787\n",
      "Iteration 1207 => Loss: 26.118020\n",
      "Iteration 1208 => Loss: 26.098453\n",
      "Iteration 1209 => Loss: 26.079087\n",
      "Iteration 1210 => Loss: 26.059920\n",
      "Iteration 1211 => Loss: 26.040953\n",
      "Iteration 1212 => Loss: 26.022187\n",
      "Iteration 1213 => Loss: 26.003620\n",
      "Iteration 1214 => Loss: 25.985253\n",
      "Iteration 1215 => Loss: 25.967087\n",
      "Iteration 1216 => Loss: 25.949120\n",
      "Iteration 1217 => Loss: 25.931353\n",
      "Iteration 1218 => Loss: 25.913787\n",
      "Iteration 1219 => Loss: 25.896420\n",
      "Iteration 1220 => Loss: 25.879253\n",
      "Iteration 1221 => Loss: 25.862287\n",
      "Iteration 1222 => Loss: 25.845520\n",
      "Iteration 1223 => Loss: 25.828953\n",
      "Iteration 1224 => Loss: 25.827287\n",
      "Iteration 1225 => Loss: 25.808387\n",
      "Iteration 1226 => Loss: 25.789687\n",
      "Iteration 1227 => Loss: 25.771187\n",
      "Iteration 1228 => Loss: 25.752887\n",
      "Iteration 1229 => Loss: 25.734787\n",
      "Iteration 1230 => Loss: 25.716887\n",
      "Iteration 1231 => Loss: 25.699187\n",
      "Iteration 1232 => Loss: 25.681687\n",
      "Iteration 1233 => Loss: 25.664387\n",
      "Iteration 1234 => Loss: 25.647287\n",
      "Iteration 1235 => Loss: 25.630387\n",
      "Iteration 1236 => Loss: 25.613687\n",
      "Iteration 1237 => Loss: 25.597187\n",
      "Iteration 1238 => Loss: 25.580887\n",
      "Iteration 1239 => Loss: 25.564787\n",
      "Iteration 1240 => Loss: 25.548887\n",
      "Iteration 1241 => Loss: 25.533187\n",
      "Iteration 1242 => Loss: 25.532213\n",
      "Iteration 1243 => Loss: 25.514180\n",
      "Iteration 1244 => Loss: 25.496347\n",
      "Iteration 1245 => Loss: 25.478713\n",
      "Iteration 1246 => Loss: 25.461280\n",
      "Iteration 1247 => Loss: 25.444047\n",
      "Iteration 1248 => Loss: 25.427013\n",
      "Iteration 1249 => Loss: 25.410180\n",
      "Iteration 1250 => Loss: 25.393547\n",
      "Iteration 1251 => Loss: 25.377113\n",
      "Iteration 1252 => Loss: 25.360880\n",
      "Iteration 1253 => Loss: 25.344847\n",
      "Iteration 1254 => Loss: 25.329013\n",
      "Iteration 1255 => Loss: 25.313380\n",
      "Iteration 1256 => Loss: 25.297947\n",
      "Iteration 1257 => Loss: 25.282713\n",
      "Iteration 1258 => Loss: 25.267680\n",
      "Iteration 1259 => Loss: 25.252847\n",
      "Iteration 1260 => Loss: 25.252567\n",
      "Iteration 1261 => Loss: 25.235400\n",
      "Iteration 1262 => Loss: 25.218433\n",
      "Iteration 1263 => Loss: 25.201667\n",
      "Iteration 1264 => Loss: 25.185100\n",
      "Iteration 1265 => Loss: 25.168733\n",
      "Iteration 1266 => Loss: 25.152567\n",
      "Iteration 1267 => Loss: 25.136600\n",
      "Iteration 1268 => Loss: 25.120833\n",
      "Iteration 1269 => Loss: 25.105267\n",
      "Iteration 1270 => Loss: 25.089900\n",
      "Iteration 1271 => Loss: 25.074733\n",
      "Iteration 1272 => Loss: 25.059767\n",
      "Iteration 1273 => Loss: 25.045000\n",
      "Iteration 1274 => Loss: 25.030433\n",
      "Iteration 1275 => Loss: 25.016067\n",
      "Iteration 1276 => Loss: 25.001900\n",
      "Iteration 1277 => Loss: 24.987933\n",
      "Iteration 1278 => Loss: 24.974167\n",
      "Iteration 1279 => Loss: 24.972047\n",
      "Iteration 1280 => Loss: 24.955947\n",
      "Iteration 1281 => Loss: 24.940047\n",
      "Iteration 1282 => Loss: 24.924347\n",
      "Iteration 1283 => Loss: 24.908847\n",
      "Iteration 1284 => Loss: 24.893547\n",
      "Iteration 1285 => Loss: 24.878447\n",
      "Iteration 1286 => Loss: 24.863547\n",
      "Iteration 1287 => Loss: 24.848847\n",
      "Iteration 1288 => Loss: 24.834347\n",
      "Iteration 1289 => Loss: 24.820047\n",
      "Iteration 1290 => Loss: 24.805947\n",
      "Iteration 1291 => Loss: 24.792047\n",
      "Iteration 1292 => Loss: 24.778347\n",
      "Iteration 1293 => Loss: 24.764847\n",
      "Iteration 1294 => Loss: 24.751547\n",
      "Iteration 1295 => Loss: 24.738447\n",
      "Iteration 1296 => Loss: 24.725547\n",
      "Iteration 1297 => Loss: 24.724120\n",
      "Iteration 1298 => Loss: 24.708887\n",
      "Iteration 1299 => Loss: 24.693853\n",
      "Iteration 1300 => Loss: 24.679020\n",
      "Iteration 1301 => Loss: 24.664387\n",
      "Iteration 1302 => Loss: 24.649953\n",
      "Iteration 1303 => Loss: 24.635720\n",
      "Iteration 1304 => Loss: 24.621687\n",
      "Iteration 1305 => Loss: 24.607853\n",
      "Iteration 1306 => Loss: 24.594220\n",
      "Iteration 1307 => Loss: 24.580787\n",
      "Iteration 1308 => Loss: 24.567553\n",
      "Iteration 1309 => Loss: 24.554520\n",
      "Iteration 1310 => Loss: 24.541687\n",
      "Iteration 1311 => Loss: 24.529053\n",
      "Iteration 1312 => Loss: 24.516620\n",
      "Iteration 1313 => Loss: 24.504387\n",
      "Iteration 1314 => Loss: 24.492353\n",
      "Iteration 1315 => Loss: 24.491620\n",
      "Iteration 1316 => Loss: 24.477253\n",
      "Iteration 1317 => Loss: 24.463087\n",
      "Iteration 1318 => Loss: 24.449120\n",
      "Iteration 1319 => Loss: 24.435353\n",
      "Iteration 1320 => Loss: 24.421787\n",
      "Iteration 1321 => Loss: 24.408420\n",
      "Iteration 1322 => Loss: 24.395253\n",
      "Iteration 1323 => Loss: 24.382287\n",
      "Iteration 1324 => Loss: 24.369520\n",
      "Iteration 1325 => Loss: 24.356953\n",
      "Iteration 1326 => Loss: 24.344587\n",
      "Iteration 1327 => Loss: 24.332420\n",
      "Iteration 1328 => Loss: 24.320453\n",
      "Iteration 1329 => Loss: 24.308687\n",
      "Iteration 1330 => Loss: 24.297120\n",
      "Iteration 1331 => Loss: 24.285753\n",
      "Iteration 1332 => Loss: 24.274587\n",
      "Iteration 1333 => Loss: 24.274547\n",
      "Iteration 1334 => Loss: 24.261047\n",
      "Iteration 1335 => Loss: 24.247747\n",
      "Iteration 1336 => Loss: 24.234647\n",
      "Iteration 1337 => Loss: 24.221747\n",
      "Iteration 1338 => Loss: 24.209047\n",
      "Iteration 1339 => Loss: 24.196547\n",
      "Iteration 1340 => Loss: 24.184247\n",
      "Iteration 1341 => Loss: 24.172147\n",
      "Iteration 1342 => Loss: 24.160247\n",
      "Iteration 1343 => Loss: 24.148547\n",
      "Iteration 1344 => Loss: 24.137047\n",
      "Iteration 1345 => Loss: 24.125747\n",
      "Iteration 1346 => Loss: 24.114647\n",
      "Iteration 1347 => Loss: 24.103747\n",
      "Iteration 1348 => Loss: 24.093047\n",
      "Iteration 1349 => Loss: 24.082547\n",
      "Iteration 1350 => Loss: 24.072247\n",
      "Iteration 1351 => Loss: 24.062147\n",
      "Iteration 1352 => Loss: 24.060267\n",
      "Iteration 1353 => Loss: 24.047833\n",
      "Iteration 1354 => Loss: 24.035600\n",
      "Iteration 1355 => Loss: 24.023567\n",
      "Iteration 1356 => Loss: 24.011733\n",
      "Iteration 1357 => Loss: 24.000100\n",
      "Iteration 1358 => Loss: 23.988667\n",
      "Iteration 1359 => Loss: 23.977433\n",
      "Iteration 1360 => Loss: 23.966400\n",
      "Iteration 1361 => Loss: 23.955567\n",
      "Iteration 1362 => Loss: 23.944933\n",
      "Iteration 1363 => Loss: 23.934500\n",
      "Iteration 1364 => Loss: 23.924267\n",
      "Iteration 1365 => Loss: 23.914233\n",
      "Iteration 1366 => Loss: 23.904400\n",
      "Iteration 1367 => Loss: 23.894767\n",
      "Iteration 1368 => Loss: 23.885333\n",
      "Iteration 1369 => Loss: 23.876100\n",
      "Iteration 1370 => Loss: 23.874913\n",
      "Iteration 1371 => Loss: 23.863347\n",
      "Iteration 1372 => Loss: 23.851980\n",
      "Iteration 1373 => Loss: 23.840813\n",
      "Iteration 1374 => Loss: 23.829847\n",
      "Iteration 1375 => Loss: 23.819080\n",
      "Iteration 1376 => Loss: 23.808513\n",
      "Iteration 1377 => Loss: 23.798147\n",
      "Iteration 1378 => Loss: 23.787980\n",
      "Iteration 1379 => Loss: 23.778013\n",
      "Iteration 1380 => Loss: 23.768247\n",
      "Iteration 1381 => Loss: 23.758680\n",
      "Iteration 1382 => Loss: 23.749313\n",
      "Iteration 1383 => Loss: 23.740147\n",
      "Iteration 1384 => Loss: 23.731180\n",
      "Iteration 1385 => Loss: 23.722413\n",
      "Iteration 1386 => Loss: 23.713847\n",
      "Iteration 1387 => Loss: 23.705480\n",
      "Iteration 1388 => Loss: 23.704987\n",
      "Iteration 1389 => Loss: 23.694287\n",
      "Iteration 1390 => Loss: 23.683787\n",
      "Iteration 1391 => Loss: 23.673487\n",
      "Iteration 1392 => Loss: 23.663387\n",
      "Iteration 1393 => Loss: 23.653487\n",
      "Iteration 1394 => Loss: 23.643787\n",
      "Iteration 1395 => Loss: 23.634287\n",
      "Iteration 1396 => Loss: 23.624987\n",
      "Iteration 1397 => Loss: 23.615887\n",
      "Iteration 1398 => Loss: 23.606987\n",
      "Iteration 1399 => Loss: 23.598287\n",
      "Iteration 1400 => Loss: 23.589787\n",
      "Iteration 1401 => Loss: 23.581487\n",
      "Iteration 1402 => Loss: 23.573387\n",
      "Iteration 1403 => Loss: 23.565487\n",
      "Iteration 1404 => Loss: 23.557787\n",
      "Iteration 1405 => Loss: 23.550287\n",
      "Iteration 1406 => Loss: 23.542987\n",
      "Iteration 1407 => Loss: 23.540653\n",
      "Iteration 1408 => Loss: 23.531020\n",
      "Iteration 1409 => Loss: 23.521587\n",
      "Iteration 1410 => Loss: 23.512353\n",
      "Iteration 1411 => Loss: 23.503320\n",
      "Iteration 1412 => Loss: 23.494487\n",
      "Iteration 1413 => Loss: 23.485853\n",
      "Iteration 1414 => Loss: 23.477420\n",
      "Iteration 1415 => Loss: 23.469187\n",
      "Iteration 1416 => Loss: 23.461153\n",
      "Iteration 1417 => Loss: 23.453320\n",
      "Iteration 1418 => Loss: 23.445687\n",
      "Iteration 1419 => Loss: 23.438253\n",
      "Iteration 1420 => Loss: 23.431020\n",
      "Iteration 1421 => Loss: 23.423987\n",
      "Iteration 1422 => Loss: 23.417153\n",
      "Iteration 1423 => Loss: 23.410520\n",
      "Iteration 1424 => Loss: 23.404087\n",
      "Iteration 1425 => Loss: 23.402447\n",
      "Iteration 1426 => Loss: 23.393680\n",
      "Iteration 1427 => Loss: 23.385113\n",
      "Iteration 1428 => Loss: 23.376747\n",
      "Iteration 1429 => Loss: 23.368580\n",
      "Iteration 1430 => Loss: 23.360613\n",
      "Iteration 1431 => Loss: 23.352847\n",
      "Iteration 1432 => Loss: 23.345280\n",
      "Iteration 1433 => Loss: 23.337913\n",
      "Iteration 1434 => Loss: 23.330747\n",
      "Iteration 1435 => Loss: 23.323780\n",
      "Iteration 1436 => Loss: 23.317013\n",
      "Iteration 1437 => Loss: 23.310447\n",
      "Iteration 1438 => Loss: 23.304080\n",
      "Iteration 1439 => Loss: 23.297913\n",
      "Iteration 1440 => Loss: 23.291947\n",
      "Iteration 1441 => Loss: 23.286180\n",
      "Iteration 1442 => Loss: 23.280613\n",
      "Iteration 1443 => Loss: 23.279667\n",
      "Iteration 1444 => Loss: 23.271767\n",
      "Iteration 1445 => Loss: 23.264067\n",
      "Iteration 1446 => Loss: 23.256567\n",
      "Iteration 1447 => Loss: 23.249267\n",
      "Iteration 1448 => Loss: 23.242167\n",
      "Iteration 1449 => Loss: 23.235267\n",
      "Iteration 1450 => Loss: 23.228567\n",
      "Iteration 1451 => Loss: 23.222067\n",
      "Iteration 1452 => Loss: 23.215767\n",
      "Iteration 1453 => Loss: 23.209667\n",
      "Iteration 1454 => Loss: 23.203767\n",
      "Iteration 1455 => Loss: 23.198067\n",
      "Iteration 1456 => Loss: 23.192567\n",
      "Iteration 1457 => Loss: 23.187267\n",
      "Iteration 1458 => Loss: 23.182167\n",
      "Iteration 1459 => Loss: 23.177267\n",
      "Iteration 1460 => Loss: 23.172567\n",
      "Iteration 1461 => Loss: 23.172313\n",
      "Iteration 1462 => Loss: 23.165280\n",
      "Iteration 1463 => Loss: 23.158447\n",
      "Iteration 1464 => Loss: 23.151813\n",
      "Iteration 1465 => Loss: 23.145380\n",
      "Iteration 1466 => Loss: 23.139147\n",
      "Iteration 1467 => Loss: 23.133113\n",
      "Iteration 1468 => Loss: 23.127280\n",
      "Iteration 1469 => Loss: 23.121647\n",
      "Iteration 1470 => Loss: 23.116213\n",
      "Iteration 1471 => Loss: 23.110980\n",
      "Iteration 1472 => Loss: 23.105947\n",
      "Iteration 1473 => Loss: 23.101113\n",
      "Iteration 1474 => Loss: 23.096480\n",
      "Iteration 1475 => Loss: 23.092047\n",
      "Iteration 1476 => Loss: 23.087813\n",
      "Iteration 1477 => Loss: 23.083780\n",
      "Iteration 1478 => Loss: 23.079947\n",
      "Iteration 1479 => Loss: 23.076313\n",
      "Iteration 1480 => Loss: 23.074220\n",
      "Iteration 1481 => Loss: 23.068253\n",
      "Iteration 1482 => Loss: 23.062487\n",
      "Iteration 1483 => Loss: 23.056920\n",
      "Iteration 1484 => Loss: 23.051553\n",
      "Iteration 1485 => Loss: 23.046387\n",
      "Iteration 1486 => Loss: 23.041420\n",
      "Iteration 1487 => Loss: 23.036653\n",
      "Iteration 1488 => Loss: 23.032087\n",
      "Iteration 1489 => Loss: 23.027720\n",
      "Iteration 1490 => Loss: 23.023553\n",
      "Iteration 1491 => Loss: 23.019587\n",
      "Iteration 1492 => Loss: 23.015820\n",
      "Iteration 1493 => Loss: 23.012253\n",
      "Iteration 1494 => Loss: 23.008887\n",
      "Iteration 1495 => Loss: 23.005720\n",
      "Iteration 1496 => Loss: 23.002753\n",
      "Iteration 1497 => Loss: 22.999987\n",
      "Iteration 1498 => Loss: 22.998587\n",
      "Iteration 1499 => Loss: 22.993487\n",
      "Iteration 1500 => Loss: 22.988587\n",
      "Iteration 1501 => Loss: 22.983887\n",
      "Iteration 1502 => Loss: 22.979387\n",
      "Iteration 1503 => Loss: 22.975087\n",
      "Iteration 1504 => Loss: 22.970987\n",
      "Iteration 1505 => Loss: 22.967087\n",
      "Iteration 1506 => Loss: 22.963387\n",
      "Iteration 1507 => Loss: 22.959887\n",
      "Iteration 1508 => Loss: 22.956587\n",
      "Iteration 1509 => Loss: 22.953487\n",
      "Iteration 1510 => Loss: 22.950587\n",
      "Iteration 1511 => Loss: 22.947887\n",
      "Iteration 1512 => Loss: 22.945387\n",
      "Iteration 1513 => Loss: 22.943087\n",
      "Iteration 1514 => Loss: 22.940987\n",
      "Iteration 1515 => Loss: 22.939087\n",
      "Iteration 1516 => Loss: 22.938380\n",
      "Iteration 1517 => Loss: 22.934147\n",
      "Iteration 1518 => Loss: 22.930113\n",
      "Iteration 1519 => Loss: 22.926280\n",
      "Iteration 1520 => Loss: 22.922647\n",
      "Iteration 1521 => Loss: 22.919213\n",
      "Iteration 1522 => Loss: 22.915980\n",
      "Iteration 1523 => Loss: 22.912947\n",
      "Iteration 1524 => Loss: 22.910113\n",
      "Iteration 1525 => Loss: 22.907480\n",
      "Iteration 1526 => Loss: 22.905047\n",
      "Iteration 1527 => Loss: 22.902813\n",
      "Iteration 1528 => Loss: 22.900780\n",
      "Iteration 1529 => Loss: 22.898947\n",
      "Iteration 1530 => Loss: 22.897313\n",
      "Iteration 1531 => Loss: 22.895880\n",
      "Iteration 1532 => Loss: 22.894647\n",
      "Iteration 1533 => Loss: 22.893613\n",
      "Iteration 1534 => Loss: 22.893600\n",
      "Iteration 1535 => Loss: 22.890233\n",
      "Iteration 1536 => Loss: 22.887067\n",
      "Iteration 1537 => Loss: 22.884100\n",
      "Iteration 1538 => Loss: 22.881333\n",
      "Iteration 1539 => Loss: 22.878767\n",
      "Iteration 1540 => Loss: 22.876400\n",
      "Iteration 1541 => Loss: 22.874233\n",
      "Iteration 1542 => Loss: 22.872267\n",
      "Iteration 1543 => Loss: 22.870500\n",
      "Iteration 1544 => Loss: 22.868933\n",
      "Iteration 1545 => Loss: 22.867567\n",
      "Iteration 1546 => Loss: 22.866400\n",
      "Iteration 1547 => Loss: 22.865433\n",
      "Iteration 1548 => Loss: 22.864667\n",
      "Iteration 1549 => Loss: 22.864100\n",
      "Iteration 1550 => Loss: 22.863733\n",
      "Iteration 1551 => Loss: 22.863567\n",
      "\n",
      "w=1.100, b=12.930\n",
      "Prediction: x=20 => y=34.93\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T15:31:36.075561Z",
     "start_time": "2025-05-21T15:31:35.607177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sea.set()\n",
    "plt.axis([0., 50., 0., 50.])\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"Reservations\", fontsize=14)\n",
    "plt.ylabel(\"Pizzas\", fontsize=14)\n",
    "plt.plot(reservations, pizzas, \"bo\")\n",
    "plt.plot(reservations, predict(reservations, w, b), color=\"red\", label=\"Prediction\")"
   ],
   "id": "11fd76df2bb55339",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x348b7a40>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAHDCAYAAAAwQ44XAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAARQpJREFUeJzt3Qd4VHUa7/E3CQRCICGBINUCgkiVIiqKiqCrCArSUVbsCOpV7Kur61WuosKigooNlWZDEERdqi4qiIKALoihSJWaQEJPmfu8f3ZmM5mTIcm0M2e+n+fhGeack5kz808mv/zLe+JcLpdLAAAA4CXe+y4AAAAUIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMBCBbGZRx99VD799FPLfc8++6xcd9115v+ZmZnyz3/+U37++Wc5evSotGrVSu655x5p165dmM8YAAA4UZzdLkvSq1cvyc3NlbvvvttnX9u2baVBgwayYcMGGTBggFSqVEkGDRokycnJMmXKFNmxY4e888470qFDh4icOwAAcA5bhaT8/Hxp06aNXHHFFTJ69OgSj7v11ltl2bJlMmfOHBOaVFZWlvTo0UOqV69utgMAADhmTtKmTZvk+PHj0qRJkxKP2bt3ryxevFi6dOniCUgqPT1d+vTpI+vXr5dVq1aF6YwBAIBT2Sok/fbbb+a2cePG5vbIkSNSUFDgdYw7ALVu3drn63VeUtFjAAAAHBWSvvnmG+ncubOcc845JgwNGzZMtmzZYvbt3LnT3NapU8fn62vXrm1ut23bFtbzBgAAzmOr1W3r1q0ztytXrjTBSOcXrVixQiZNmmRuP/74Yzl48KA5pkqVKj5fX7lyZU8PFAAAgGNCUvfu3c2Q2dChQyUxMdFsu/zyy81kbl3tppO5mzZtetLHiYuLK/c56Dz2QL4eAAA4g61CUs+ePS2362o3HV779ttvPXWQtDZSce4epJSUlHKfgwaknBydC1VY7sdAcCQkxEtKShLtYQO0hX3QFvZBW9hHamqSxMfHOzsk+VOjRg3ZvXu31K9f32tuUlH+5iuVhX6z5+fzDW8XtId90Bb2QVvYB20ReaEqZmSbidu6tF/rHGnV7OLy8vJk8+bNZsl/y5YtTVpcvXq1z3HubTo8BwAA4IiQpD1FWiNp4cKFsnbtWq99EyZMMFW4e/fuLTVr1pSOHTvK3LlzZevWrZ5jtJjk9OnTzZylZs2aReAVAAAAJ7FVxe0lS5bIbbfdJklJSeZyI7Vq1ZKlS5eaQHTeeefJW2+9ZSZ0//7779K/f39zOZIhQ4aYbe7LkkycOFHat28f0HlkZx+i69QGKlSIl7S0ZNrDBmgL+6At7IO2sI/09GQzR8zRIUn98ssvMn78eFm+fLmZiK1DbNdcc43ccsstnhVvSnubxowZY47T4bcWLVrIvffea2orBYpveHvgA8g+aAv7oC3sg7awj5gJSXbAN7w98AFkH7SFfdAW9kFbOD8k2WZOEgAAgJ0QkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACwQkgAAACxUsNoIAChZYaFLft+6X/YfOibVkytJkwbVJT4+LtKnBSDICEkAUAbL1+2WqfMzJTv3mGdbWrVKMqhrY2l3Vq2InhuA4GK4DQDKEJDGz/jVKyApva/bdT8A5yAkAUAph9i0B8mfafMzzXEAnIGQBACloHOQivcgFZeVe8wcB8AZCEkAUAo6STuYxwGwP0ISAJSCrmIL5nEA7I+QBACloMv8dRWbP+nVTpQDAOAMhCQAKAWtg6TL/P0Z2LUx9ZIAByEkAUApaR2k4b1a+PQoaQ+SbqdOEuAsFJMEgDLQINSmcQYVt4EYQEgCgDLSQNT0tLRInwaAEGO4DQAAwAIhCQAAwAIhCQDKKO7AfsmolWL+xe/YHunTARAihCQAKIMKK1dIzcaneu4TkgDnIiQBQClVfvsNSbviUs/9Q/c9IPntO0T0nACEDqvbAKAUUgdcJ4kL53vu7//4M8m7pHNEzwlAaBGSAMCfvDzJqFfDa9O+1euksHadiJ0SgPBguA0AShC/fZtPQNqzI4uABMQIQhIAWEic/y+p0aaZ5/6xq7rLnt05IhXogAdiBSEJAIpJ/sfjkjqor+d+7uiXJee9qRE9JwDhx59EAODmcknGKalem7IWficFLVpG7JQARA49SQAgIgnrfvMJSHs3bCMgATGMkAQg5iU//rCkd/Kud7Rn1wFxVUuJ2DkBiDyG2wDENL20SHFmgjaAmEdPEoCYVTwgFTQ4lYAEwIOQBCD2HDzoE5ByJrwjWct/jdgpAbAfW4ekgoICGTRokJx11lmSn5/vtW/Hjh3y0EMPyUUXXSStW7eWfv36yYIFCyJ2rgCiQ+IXn0tGw7pe2/au3yrHevWJ2DkBsCdbh6TXX39dli9f7rN9z549csMNN8j8+fOld+/e8vDDD5sQNWzYMJk9e3ZEzhWA/aV1vlBShwzy2qbDa64U71VtAGDridurV6+WV199VRITE+X48eNe+8aNGyfbt2+XqVOnSrt27cy26667Tvr27SsjR46ULl26SJUqVSJ05gDsiAnaABzRk3To0CF54IEHpFOnTnLOOef4DMHNmjXLDLG5A5KqXLmyDB48WLKzs+Xrr7+OwFkDsG2ByGIB6cjgIQQkANEZkrQ3KDc3V5555hmffZmZmXL48GETkopr1aqVuV21alVYzhOAvcVv2uhbQXvBt3Jw9MsROycA0cN2w21z586V6dOny/jx46VmzZo++3ft2mVu69TxvQp37dq1ze22bdvCcKYA7KzKc09L8pgXvLbt2blfJN6WfxsCsCFbhSQNQH//+9+lT58+0rVrV8tjtIdJWc05SkpKMrdHjhwJ6DwSEvgQtQN3O9AekRdtbZGWXtVnW3bWQXt94MVIWzgZbWEfcXGheVzbfGa4XC6zSq1atWryt7/9ze9xJ9sXF+C7lZJyImzBHmgP+4iKtij+85+WJpKVJWniLFHRFjGCtnAu24SkiRMnytKlS80w27Fjx8w/lZeXZ273798vFStWlOTkZHP/6NGjPo/h7kFKSQnseks5OUekoKAwoMdA4PSvM/3woT0iLyra4sgRSauX4bXp0CuvyfHrB4tkHxKniIq2iBG0hX2kpiZJfAiG0m0TkhYtWmR6grTWkZULL7xQ6tWrZ2onqZ07d5Y4X8k9N6m89Js9P59veLuI1vYoLHTJ71v3y/5Dx6R6ciVp0qC6xMeHqE84xtui4sL5Un3AdV7b9v62SVzpNURseL5ObotYRFtEnp9BJmeEJB1qy8nxXZL73HPPybp16+Ttt982c44aNmxohuS0jlJx7lVtbdu2Dcs5AyVZvm63TJ2fKdm5J3pEVVq1SjKoa2Npd1atiJ6b06Rec6UkLv3eaxvL+wE4KiS1aNHCcntq6onlu+eff75UqHDidLt16yYfffSRrFixwhOIdPht8uTJZkXcxRdfHMYzB3wD0vgZvtcA08Ck24f3akFQChIKRAIIpaickn/33XebMHTbbbeZ6ttaeVuv8aY1lB577DGpVKlSpE8RMUqH2LQHyZ9p8zPNcQhuQDraqzcBCYAze5LKIiMjQ6ZNmyajR4+W999/30zu1ovgTpgwQS655JJInx5imM5BKjrEZiUr95g5rulpTltvFR7x27ZKjbbNvbZlf7lA8tudG7FzAuBMtg9JkyZNstzeoEEDGTt2bNjPB/BHJ2kH8zh4S3p5jFR95h9e2/bsyBL571A8AAQTnyxAEOkqtmAeh/9h/hGAcIvKOUmAXekyf13F5k96tRPlAFD+gOSKjycgAQg5QhIQRFoHSZf5+zOwa+Oor5cUNseP+wSk3GdfkL16DTYACDFCEhBkurxfl/kX71HSHiSW/5de4uezJKO+90Wu9/6SKUdvuSNi5wQgtjAnCQgBDUJtGmc4ruJ2uNQ4s4HE5xzw2sbwGoBwIyQBIaKBiGX+ZccEbQB2wXAbANsGpMLU6gQkABFDSAIQcfGb//AJSAfeniT7MrdE7JwAgOE2ABGV/PjDUuWN17y27dmyW6Ry5YidEwAoQhLgMHpduEhMGC/t8xY9rselTXz2M7wGwC4ISYCDLF+321xgt+j147QUgdZuCmXpgdI+b9HjZo/p6fM4BCQAdsKcJMAhNICMn/GrzwV29b5u1/2RfF73cbn7D/oEpPnNL5OvFq8PyfkBQHkRkgAH0CEs7aHxZ9r8THNcJJ43P7/QHHfpmq9lxkt9vfYPHTJeXvrLPSE5PwAIBMNtgAPoHJ/iPTnFZeUeM8cFs3ZTaZ934Ypt8v5TV/ns6zFiZkjPDwACQU8S4AA6CTqYxwX7ea/v3sJvQCrr4wFAOBCSAAfQ1WTBPC6Yz2s1QdsqIJX28QAgXAhJgAPocvviF9QtTi+wq8eF63lPObDLJyC90+nGEgNSKM4PAAJBSAIcQOsR6XJ7fwZ2bRz0ekklPe+IL/4pb719h9e2eV/9LDPO7RXW8wOAQBCSAIfQekTDe7Xw6dnRHhrdHqo6ScWfV3uPOv/2jU/9o3PaNorI+QFAecW5XC7W3BaTnX3ILFlGZFWoEC9pacm0hw0qbpemLfR5T6mdetICkZGqCO4U/FzYB21hH+npyZKQEPx+H0oAAA6jgSPsy+jz8uSUejW8NuWf2Viyv19uj/MDgHJguA1AQJImjJeMYgEpe848y4AEANGEniQA5ZZRK8Vn255dB0TiGD4DEP0ISQCCF5C4QC0AB2G4DUCZEZAAxAJCEoBSi9+00ScgHb+sKwEJgCMx3AagdNq2ldSff/batG/Vb1JYp27ETgkAQomQBOCk0tKr+myj9wiA0zHcBsAv5h8BiFWEJADWCgsJSABiGiEJgI9KH0yRjNrVvTe+955kZx2M1CkBQNgxJwlwmECvjWbVe5S9N0fSalQTyT4kTsZ15QAURUgCHGT5ut0ydX6mZOce82xLq1ZJBnVtLO3OqnXSry9peK1CvPM7nQN97wA4j/M/+YAYob/kx8/41euXvNL7ul33+xPL848Cfe8AOBMhCXDIMJH2gvgzbX6mOa64+D93+ASkvNZtYiYgBfLeAXA2QhLgADqPpngvSHFZucfMcUWl9u8lNVo39dq274eVsn/eNxIryvveAXA+5iQBDqATjct6XCwPrwX63gGIDfQkAQ6gK7HKchwBqfzvHYDYQUgC/kvnnPy2OVuWrtlpbqNpDoouVdeVWP6kV6skTeqnEpDK+941KFY3CoDjMdwGOGD5t9by0XPVlVgl+T+Jm+SUOld5bTs4cpQcue1OiWWlee8Gdm1MvSQgBsW5XK7o+XM5TLKzD0l+fmGkTyPmVagQL2lpySFvD/fy75IM79UiKoJSSWFPe0Hee8o7HKk92/eJVKxoq7aw43unAclO7R8LbREtaAv7SE9PloSE4A+O0ZOEmFba5d9tGmdERU+C/jLXcy1aNbrTuaf5HBfLw2tlee+ouA3ENkISYlpZln83PS1NooH+UnefK/OPyv/eAQATtxHTnLr8O27fPp+AVFC3HgEJAMqAniTENCcu/0675AKpsPY/Xtuyvl4iBc2aR+ycACAaEZIQ09zLv/0NuUXT8m+G1wAgeBhuQ0xzL//2p/jyb7vWU4p0QLLr+wIA5UVPEmKermrSZf6lWf5t13pKkQ5Idn1fACAQ1EmyQM2L2KxBoj0f/pZ/27GeUqWZ0yXl9pu8th3t019yX30zbG1hx/fFyajNYx+0hX1QJwmI4PJvO9ZTsuw92vSnSHKyhIsd3xcACBbmJAFBrqcU0eG1MAYkO74vABBMhCQgyuopRXr+kV3fFwAINkISECX1lOL2Z9sqINnlfQGAUCEkAWWop+RPKOsppfa9Vmo28b4G2/4PPo14DaRIvy8AEEqEJCBE9ZSCRXuPEr9Z5LVNw1HeZV0llt8XAAg1QhJQxnpKxXtOtKckVMvc7Ta8Zpf3BQDCgRIAQBnoL3xdzu6vnlIsBaRIvC8AEC6EJCCI9ZSCoeLC+VJ9wHVe245fepkc+GimxPL7AgDhRkgCbMSq92jv75vFVZ3wAQAS63OStmzZIiNGjJBOnTrJOeecI3379pXZs2f7HJeZmSnDhg2TCy64QNq0aSM33nijLF++PCLnDIRyeI2ABACRYauepO3bt0u/fv2koKBABg8eLDVq1JAvvvhCHnjgAbNv6NCh5rgNGzbIoEGDpFKlSua45ORkmTJliglK77zzjnTo0CHSLwU46bXgonX+EQDECltd4Pb++++XOXPmyIcffiitW7c22zQw9enTxwSjxYsXS2pqqtx6662ybNkyc2yDBg3McVlZWdKjRw+pXr262R4ILlZoD9F88Ui96Kte06zoJTt09Zcul/da7XXokGScUcf2ASma28JpaAv7oC2cf4FbWw23xcfHS+fOnT0BSSUkJMj5558vx44dM0Fp7969Jix16dLFE5BUenq6CVPr16+XVatWRegVACcC0vgZv/pc00zv63bdr6rdeatPQMp5813bBSQAiFW2Gm574YUXLLevWbPGBKi6det6AlDRIOXWqlUrc6vHWO0HwjHEpj1I/kybnylXdjrTZzvhCADsxVYhqajc3Fz5448/ZPLkybJ06VK54YYbpHbt2rJgwQKzv04d3yEK3a+2bdsW0HOHossO5W+HaGqPtX9k+fQgFffeU1f5bMvOOmjfH8YobQunoi3sg7awj7gQlWSz7efygw8+KIsWnbgUg/YKDR8+3Pz/4MGD5rZKlSo+X1O5cmVze+TIkYCeOyUlKaCvR3BFU3vkbcr2u3/2mJ6+G10uiZb1a9HUFk5HW9gHbeFctg1JuvRf5xj98ssv8u6778q1115rVrCVZp55XICRMifniBQUMAkv0vSvM/3wiab2qBhn/f3ZdMdv8sIHj3hty2/WXHK//UEk+5DYXTS2hVPRFvZBW9hHamqSmZYTMyFJJ2arrl27SsuWLU1P0iuvvOKZd3T06FGfr3H3IKWk+C6nLgv9Zmelgn1EU3s0qptqVrEVHXKz6j3as/p3HR8WiZLXFY1t4XS0hX3QFpEXqnX6UTGQqoGpatWq8uuvv0r9+vXNtp07d/oc595mNV8JCAetg6TL/P0FpK8Wrz8RkGDLife/bc6WpWt2mlu9DyB22aYnSZf2X3/99dKiRQsZPXq01768vDxTAiApKcn0KmmX2urVq30ew71NK3ADkaJ1kIb3amG5gk0DkledJERfbSsAMcM2PUk1a9Y0c4nmzZtnah0VpVW0NSjp0Jse17FjR5k7d65s3brVc4wWk5w+fbo0bdpUmjVrFoFXAPzX8eOWAWnXzgP8so3y2lYAYoutKm7/8MMPppq2Dq1pr1JaWprZ9q9//Uvatm1rJnDrpUh+//136d+/v7kcyZAhQyQxMdFM6t6xY4dMnDhR2rdvH9B5UD3VHqKxmm3Ve4dL0tRJXtsO/mOkHBl2t0SzaGyL0tIhtQdf+95v6Yb0apXk+Ts7lnhZmXBycltEG9rC+RW3bRWS1H/+8x8ZN26c/Pjjj2ZytlbV1suNaHjSMOS2du1aGTNmjLmorQ6/6TDdvffeay6KGyi+4e0h2j6ALK+/tutA6Ap4hFG0tUVZ6Nyj56f9fNLjHhrYRpqeFvliDU5ui2hDWzg/JNlmTpJb8+bN5bXXXjvpcWeffba8+eabYTkn4GS4QG300gsQB/M4AM5hmzlJQLQiIEW36smVgnocAOcgJAHlVOGXVQQkB2jSoLpZxeaPzknS4wDEFkISUA4ajtK6dPLalrV0BQHJAbWtrAzs2tgWk7YBhBchCSijknqPChr6LvtHdNW2Kt6jpD1Iup3SDUBsCnji9vHjx01dI3dtIi3o+Pbbb0uFChXkr3/9q7k4LeAUDK85lwahNo0z5Pet+80kbZ2DpENs9CABsSugkPTnn3/K4MGDTV2jmTNnyu7du+XGG2/0XENNC0Nq/SKtkg1EtYICyajju/ybgOQsGojssMwfgAOG2/SCs7t27ZKePU9cn0qDkgakUaNGyVdffSW1atWSN954I1jnCkRE8tNP+gSkw/eMICABgMMF1JP0/fffm8rYWvVa/fvf/zaXDbn22mvN/X79+pkK2ICjhtf+zBZJSIjI+QAAoqQnad++fdK48YlVIdqDtHLlSunQoYNnf3p6uhw+fDjwswTsNP+IgAQAMSGgnqSMjAwTlNSSJUskPz/fXHzWLTMz0xwDlOd6Wmv/yJK8TdlSMc4ljeqmhnwCrT6ne9Juj0ub+OxneA0AYktAIalVq1Yybdo0OfXUU80lQnRFW+fOnU1Y0knbH3/8sVx11VXBO1vEBL3i+tT5mV4XHNWl2VrLJlRLsd3PWXnLJnlj4jCf/QQkAIg9AQ233X///eZWLyyrF6a9/fbbpUaNGubitPfdd5+kpKTInXfeGaxzRQzQsDJ+xq8+V2TX+7pd94fqOd9/6iqfgHTv9aPlq8Xrg/6cAACH9yQ1aNBAZs2aZSZw16lTx/QsKZ2npMGpb9++JjQBpR3u0t4cf6bNzzS1bII19OZ+ztljTqzQLKrHiJkheU4AQIwUk6xWrZr85S9/8dqmK9yGDh0a6EMjxuh8oOI9SMVl5R4zxwWrlo0+lvYglRSQQvGcAIAYCUn79++XZcuWycGDB6WwsNCzvaCgQHJycuTbb7+V9957L9CnQQzQCdPBPO6kXC7pdO5pfgNS0J8TABAbIUnnIWmF7UOHDnm2uVwuiYv737BExYoVAztDxAy9DEQwj/OnyovPSfLz/89r2/dnni/PXvNIyJ4TABBDIUkrbh89elRuvvlms7JNq2s/+eSTpnfpk08+kaysLJkzZ07wzhZRr+gy++LXxtL/6yo2f0NuesFRPS7Y9Y963fOx5FeoGLLnBADEWEjS4pHXXXedPPjgg6ZopJYBaNiwoZx33nnSv39/ueaaa0zF7cceeyx4ZwzHLu3XsKT/15VmJRnYtXFAE6itApKuXssP4XMCAGKwBIDOQ2rRooX5f5UqVaR27dqydu1aT7Xt3r17y3fffRecM0VMLO3XsDS8VwsTnor35uj2QOoklVRBO5TPCQCI0Z6kqlWrSl5enud+/fr1ZcOGDV4lAnbu3BnYGSLmlvZrKNH/b9hxQPJccQFX3I7fvk1qtGnmt0Ck+zlLGgoEAMSegEJS8+bNTWVtvcitOuOMM2TFihWe/Vu2bJHExMTAzxIxt7Rfw8nZp6dLWlqyZGcfkvz8/62cDLT3aP9HMyXv0st8tutzsswfABCU4bZ+/frJ0qVLzbyk3NxcufLKK8312rQS92uvvSaTJk3yDMchdoV9af9JhtesAhIAAEHtSdIiko888ohMmDBBkpKS5IILLpCrr77as6KtevXqnkuXIHaFc2n/yQISAAClFefSwkYB0iKS8fH/65Ravny5ZGdnS7t27SQtLfqGLwIZ3oH1nKQHX/v+pEv7n7+zo9ccoAoV4ss13FbegOSvPEGsK29bIPhoC/ugLewjPT1ZEhICGhwLfk/Sjh07pFatWqZGUlEajtS6detkypQpctdddwV2lohq4VjarypPfEuqPTzCa1teu3Nl/5cLAi5PAACIPQHFrssuu0z++te/ml4jKxqSxo8fH8hTwCFCvcxee4+KB6S9G7eXOiCVpjwBACC2BHztNl3NpoUjX3/9dVNIEihJqJbZBzL/qKzlCQAAsSPgAbyBAweaC9kOGDBAlixZEpyzgmO5l9mf36y2uY1kQCpreQIAQGwJOCS1bdtWPvjgA0lNTZXbbrtNPvroo+CcGeBH3L59QVnBFqnyBAAA+wvKVPDTTz9dPvzwQ2nWrJm5wO2oUaPM9uITuoFgSG/TTGqefYbXtpw3JpZriX8kyhMAAKJD0FKMXqtNi0dqXSS9qO3WrVvlqquuCtbDAyGpf6RzonQy+cnKE+hxAIDYEtSiApUqVZJXXnlFBg8eLPPnz5eRI0cG8+ER40JRINJdnsCfYJQnAABEn6BXXoqLi5PHHnvMVOLev5/JrrB/Be1QlycAAMTgcNuCBQvMMJuVIUOGSKNGjWTVqlWBPAVi3YcfStqAAV6bCurWk6yVa6OiPAEAIEZD0o8//ijnnnuu1KtXz3J/QkKCLFu2LJCnQAxLS6/qs23v2k3iqlEjpOUJAAAIeLhNh9T69esnK1eutNy/d+9eE6SAYA2vhSogAQAQ9DlJBw4ckBtvvFHmzJkT6EMBIZ9/BABA2ELSAw88IE2aNDG348aNC/ThEMsOH7YMSNlZByNyOgCA2BZwSKpRo4ZMnjzZXOxWL2arYen48eOelW6IDXoNtN82Z8vSNTvNrd4vi2rDb5eM02t7bTv41rsirrI9TiRfAwDAWSoEqz6S9iJpXSQNTDt27DD3qbgdG5av220uElu0IKMup9f6Q6VZPl/S8FqFCkGvUBGy1wAAcJ6g/RbSXqPHH3/cTObWidx64dtdu3YF6+FhUxouxs/41aditd7X7brf7vOPAn0NAABnCvqf6lofaezYsbJz50554YUXgv3wsBEdjtLeF3+mzc8scdjKDgEp0NcAAHCugEJS3bp1pUqVKj7br7jiCnn33XelWrVqgTw8bE4LL/q75pnKyj1mjiuqwo8/+ASk/LObRWQFW3lfAwDA+QKaNLRw4cIS97Vp00Y+++wz2bx5cyBPARvTytRlPc6q92jfL79L4Snek7bt/BoAALGhTCGpsLBQ4uPjve77k5GRYf7BmfTSHWU5zg7Da4G+BgBA7ChTSGrevLk8//zz0qNHD3O/WbNmJ13mr/vXrFkT2FnClvTaZroCzN9wlV4kVo+zY0Aq62sAAMSWMoWk9u3bS82aNT339bptcDadsFzSRV/1VpfI6wqwkgy69HQ5pXaqLQNSaV/DwK6NudAtAMSgOJerfNX6cnNzJT8/X9LSnHdB0OzsQ5Kf738oMRaUtnaQ1XHa+/L41q+k8TsveT1mzvg35FjfAaV6fq2TlJaWHJb2KOk1aECiTlJ42wL+0Rb2QVvYR3p6siQkxEc+JH333XcyatQoycw8sWy6fv36Mnz4cOnZs6c4Bd/w/6sdVJLhvVp4hYfiPU6dzj3N52v27Dqg46+2/QDy12sW6/hlYB+0hX3QFs4PSWUabvv555/ljjvukIKCAjnzzDMlISFBNmzYII8++qgcO3ZM+vfvH/QTRPiVtnZQm8YZXkNvTU870ato1/lHJ1P0NQAAUKbY9fbbb0tKSopMnz5dZs+eLTNnzpSvvvrKXOBWr9sGZwikdlC0BiQAAAIKSatWrZLrr7/erGpzq1evntx3332yZ88e2bp1a1keDjZVntpB8Zs2WhSIbE5AAgBErTINt2VnZ5sq28WdffbZolObdu/eLQ0aNAjm+SEKagdVv/wSqbjqZ699+1b8Rwrr870AAIiRkKSr2SpU8P2SxMREc3v8+PHgnRliuv6Rzota+0eW5G3KlopxLmlUN5VJ1ACA6LksCZyptLWDQlX/qLSlBwAACKXgr5eDI2gY0WX+Gk6K9yANv7aZXNnpzJAFJA1nxXux9L5u1/0AANiyJ+mnn34yJQCKOnTokKeG0q5du3y+xkk1lGItKOky/6K1g1ot+VJSL7nK67icl1+TYwOuj0jpAQAAQqVMxSSbNm1a4rXa9GGK73NvW7t2rUQTCoNZs5x/tHO/js8F5fF/25wtz0/zngBu5aGBbahnFGYUzbMP2sI+aAv7sEUxybvuuivoJ4DoEI76R+UpPQAAQKjYLiStW7fOFKZctmyZHDx4UDIyMqRLly5yzz33mEKWbnpZlH/+85+mCvjRo0elVatW5ph27dqF/BxjTbgKRJa19AAAADGzum3jxo0yYMAAU2Zg0KBBUqdOHVm5cqVMmTJFli5dKh9++KEkJyebS6Ho/kqVKsngwYPNNj3mxhtvlHfeeUc6dOgQ6ZfiCHG7d0vNFt4TtPNatpb9CxZHvPQAAAAxFZKeeeYZycvLk08++UQaNWpktmlo0grfI0eOlMmTJ5trxz377LPmWnGffvqpp3hljx49zL+nnnpK5syZE+FXEv2q3jtckqZO8tq274eVUnhGw4iXHmDSNgAgpkoAaOj58ccfpX379p6AVHx1nA7B7d27VxYvXmyG4IpW905PT5c+ffrI+vXrzeVTENjwWvGApMNroQxIpSo90KsFdZIAALHXk1SxYkX54osvpLDQd4WABiOVkJDgCUCtW7f2OU7nJSk9xmo/Ts4OF6h1lx7YsOOA5LniqLgNAIjtkBQfH1/idd/efPNNc3veeefJzp07zf91vlJxtWvXNrfbtm0L6FxCsYzQ9lwuSatRzWdzdtbBiH2TtGhUU1JSkiQn54gUFLC8NpLcPxMx+bNhM7SFfdAW9lFCdSLnhKSS6Lwj/aehqH///maCtqpSpYrPsZUrVza3R44cCeg59RdzTJk3T+SKK7y3TZwoMmSI2KEaUcy1h43RFvZBW9gHbeFctg5JOoH7iSeeMIHo5ZdflqpVq5oClSdTUsHL0oqlnou09Ko+27J379cqaSLZJyqpR4r+dUZPkj3QFvZBW9gHbWEfqalJZkQqZkKShiKtl6TBaMKECZ75RrrcX2ltpOLcPUhF6ymVh36zx0L1VL/zj2z0+mOlPaIBbWEftIV90BaRV/prh0R5SNISAI8//rjMnDlTatWqZeYj6eVQ3OrXr29u3XOTivI3Xwn2m6ANAICd2Wq2mV44d8SIESYgNWnSRD766COvgKRatmxputRWr17t8/XubW3atAnbOUebuNwcn4CU1+F8AhIAAHYOSWPHjpW5c+eaoTWdoG3VI1SzZk3p2LGjOW7r1q2e7VlZWTJ9+nQTqrT4JHwljX9ZajY60RPntu+nX2T/53Mjdk4AANiVbYbbNPDoJUV00vXll18uixYt8jmmRo0actFFF8nDDz9sVroNHDhQhgwZIomJiSZU5eTkyEsvvRSR87c7htcAAIjSkKRVtPPz883/R48ebXlM27ZtTUjSobipU6fKmDFj5NVXXzXDby1atJBRo0bJOeecE+Yztz8CEgAAZRfnKs2a+hiTnX3IMSsVojkgVagQL2lpyY5qj2hFW9gHbWEftIV9pKcnh6Sop63mJCF4En5Z7ROQcl59M2oCEgAAkWab4TYET/VuXaXiT8u8tu3ZtlckMdFzv7DQJb9v3S/7Dx2T6smVpEmD6lwbDQCAIghJDlOa4bXl63bL1PmZkp17zLMtrVolGdS1sbm4LAAAYLgtJgPS+Bm/egUkpfd1u+4HAACEJGc4etQnIB2+/U6fgKRDbNqD5M+0+ZnmOAAAYh0hKcolfj5LMk71HiLb9+NqOfTMKJ9jdQ5S8R6k4rJyj5njAACIdcxJimI1Gp8q8Qe8A42/1Ws6Sbs0SnscAABORkiKofpHuoqtNEp7HAAATsZwmwMCUmG1lFLVP9Jl/rqKzZ/0aifKAQAAEOsISVEkfstmn4B04O33Zd+GbZ77Oun6t83ZsnTNTnNbdBK21kHSZf7+DOza2Bb1kvS81/6RJd+s2GZumUwOAAg3htuiRPLjD0uVN17z2rZny26RypXLVP9Ib4f3auFznPYgDbRJnSTqOAEA7IBrt1mw23V4ylL/qCQajIoGDLtW3C7r60B4cI0q+6At7IO2cP612+hJsrnSBKTS1j9q0zjDE4T0tulpaWIn5XkdAACECnOS7CovzycgHR1wveUEbafUP3LK6wAAOAM9STaUuGCupA7s47Ut6/vlUnBmY0fXP3LK6wAAOAMhyWbSz20lCZv/iMn6R055HQAAZyAklVFpJzyXZ2J0eQpEFq1/5G+oqnj9IztO3C7P6wAAIFQISSFYml6eJezlDUhF6x/5WxVWtP6RXZfYl/V1AAAQSkzcLuPS9OK9HHpft+v+shznFrdvn09Aynn5tVIHJDd3/aPiFbW156Xosvmynl+4lfZ1AAAQavQkBXFpeutGNcu0hL3ylPel2n13ee3fs+lPkeTkcp2nBgh97JKG0aJlib37dWzYcUDyXHFSMc4ljeqm0oMEAAgrQlIQl6YvXLGt1EvYL7y8tcTv917KXtbeIyv+6h+VZYl9pGso6es4+/R0CrUBACKGkBTEJee79x8p1XGdzj3NZ1swAtLJsMQeAIDSY05SEJec16qe5Hd/nKtQZo/p6bXt0IiHwhKQFEvsAQAoPXqSgrg0/bK29eVfP261PO6M3Rvl5ckjvLbt+2GlHG9wuixctsX0QmnI0sfQ6wFF8nWwxB4AAHqSyrQ03R9dmq7hxuq44fPG+wSkPbsOyAebCuSO0V/LBwvXy8IV282t3v9oof/J1aF+HUyQBgCAkBT0penFj9PhtSt/mef1NTq89tGi9fLVsq3icnk/j97X7aEKSiyxBwCgdOJcruK/puFvNVVZKm6fUjvVa1t+85aSveg789jaY+TvnY+LE5lw/6UhG3qzY8Xt4vS1s7rNHmgL+6At7IO2sI/09GRJSAj+70vmJAVxib1HXp6cUq+G16YD706V4926m/9rqYCTRVPdr8dd0eFUidjrAAAghhGSgix+21ap0ba517Y9G3eIVK1a5lIBpT0OAAAEH3OSgihx3ldeAenYld1OLO8vEpBKUyqgrMcBAIDgIyQFSfI/HpfU6/t57ue+MFZy3v/A8lhd5q9zjvzR/XocAACIDIbbAuVySXq7FpKwbatnU/aCxZLfsrXfyX5/ObeBWcVWEt0fqknbAADg5AhJAYjLzZGajbx7e/Zu2Cauaikn/dp+l52oV6TFJ4tO4tYeJA1I7v0AACAyCEnllPDLaknvcpHnfkG9+pK14j8nUk4paRC67uJGZhVbOCpuAwCA0iMklUPiv76U1MH9PfcPD71LDv3f/1eux9JAFKpl/gAAoPwISeVQ6ZMPPf8/MOUjOX75lRE9HwAAEHyEpHI4fO8DUtjgVDly821SWL9BpE8HAACEACGpHAqat5BDzVtE+jQAAEAIEZJCJBqujQYAAEpGSAqB5et2y9T5mZKde8yzLa1aJRnUtbG0O6tWRM8NAACUDmvNQxCQxs/41SsgKb2v23U/AACwP0JSkIfYtAfJn2nzM81xAADA3ghJQaRzkIr3IBWXlXvMHAcAAOyNkBREOkk7mMcBAIDIISQFka5iC+ZxAAAgcghJQaTL/HUVmz/p1U6UAwAAAPZGSAoirYOky/z9Gdi1MfWSAACIAoSkINM6SMN7tfDpUdIeJN1OnSQAAKIDxSRDQINQm8YZVNwGACCKEZJCRANR09PSIn0aAACgnBhuAwAAsEBIAgAAsEBIAgAAsEBIAgAAsEBIAgAAsEBIAgAAsEBIAgAAsEBIAgAAsEBIAgAAiLaQtGrVKmnWrJl8//33Pvt27NghDz30kFx00UXSunVr6devnyxYsCAi5wkAAJzHtiHpjz/+kOHDh0tBQYHPvj179sgNN9wg8+fPl969e8vDDz8s+fn5MmzYMJk9e3ZEzhcAADiLLa/dNm/ePHnsscfkwIEDlvvHjRsn27dvl6lTp0q7du3Mtuuuu0769u0rI0eOlC5dukiVKlXCfNYAAMBJbNeTdPvtt8tdd90lGRkZ0r17d5/92rM0a9YsM8TmDkiqcuXKMnjwYMnOzpavv/46zGcNAACcxnYhaePGjTJixAiZMWOGnH766T77MzMz5fDhwyYkFdeqVSvPXCYAAABHDbd98cUXkpiYWOL+Xbt2mds6der47Ktdu7a53bZtW0DnkJBgu+wYk9ztQHtEHm1hH7SFfdAW9hEXFyMhyV9AUrm5uebWas5RUlKSuT1y5EhA55CScuJxYA+0h33QFvZBW9gHbeFctgtJJ+NyuU66Ly7ASJmTc0QKCgoDegwETv860w8f2iPyaAv7oC3sg7awj9TUJImPD36PXtSFpOTkZHN79OhRn33uHqSUlJSAnkO/2fPz+Ya3C9rDPmgL+6At7IO2iDw//ScBibqB1Pr165vbnTt3ljhfyT03CQAAIGZCUsOGDaVatWqyevVqn33uVW1t27aNwJkBAAAnibqQVKFCBenWrZusWLHC/HPT4bfJkydLzZo15eKLL47oOQIAgOgXdXOS1N133y0LFy6U2267TW666SZJT0+XTz75xNRQGjNmjFSqVCnSpwgAAKJcVIYkrcY9bdo0GT16tLz//vuSl5cnZ511lkyYMEEuueSSSJ8eAABwgDiXvzX1MSo7+xArFWygQoV4SUtLpj1sgLawD9rCPmgL+0hPTw5JUc+om5MEAAAQDoQkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAp4Wk7Oxsefrpp6Vz587SqlUrueaaa+STTz6J9GkBAAAHqCBR6vDhw3LzzTdLZmamDBo0SBo2bChffvmlPPbYY7J3714ZOnRopE8RAABEsagNSZMnT5Y1a9bIiy++KD169DDb+vXrJ7feequMGzdOrr32WqlTp06kTxMAAESpqB1umzlzpmRkZEj37t092+Lj4+WWW26RvLw8mT17dkTPDwAARLeoDEm5ubmyceNGMw8pLi7Oa1/r1q3N7erVqyN0dgAAwAmicrht165d4nK5LIfTqlatKsnJybJt27ZyP35qapK4XAGeJALmzr+0R+TRFvZBW9gHbWEf8fHeHSYxHZK0J0lVqVLFcn9SUpIcOXKk3I+vw3awD9rDPmgL+6At7IO2cK6obFntRTrZ/uLDcAAAAI4PSTqcpkrqLdLtKSkpYT4rAADgJFEZkurVq2d6inRuktVQnNZQql27dkTODQAAOENUhiSdnN2oUSP55ZdffPatWrXK3LZt2zYCZwYAAJwiKkOS0kuQ/Pnnn/L55597thUWFso777wjiYmJcvXVV0f0/AAAQHSLc51sFrRNHT16VHr37i2bN2+WwYMHyxlnnCFffPGFLFmyRB566CFTVBIAACDmQpLKysqSMWPGyMKFC+XQoUMmKA0ZMkR69uwZ6VMDAABRLqpDEgAAQKhE7ZwkAACAUCIkAQAAWCAkAQAAWCAkAQAAWCAkAQAAWCAk/Vd2drY8/fTT0rlzZ2nVqpUpVvnJJ59E+rRihlZKb9asmXz//fc++3bs2GFqX1100UXSunVr6devnyxYsCAi5+lk69atk3vuuUfOP/98adGihflZeOaZZyQnJ8fruMzMTBk2bJhccMEF0qZNG7nxxhtl+fLlETtvJ9qyZYuMGDFCOnXqJOecc4707dtXZs+e7XMcbRFeBQUFMmjQIDnrrLMkPz/fax+fU6H36KOPmvfe6t+nn34akp8LSgCImGu9XX/99eaN1R+Ahg0bypdffilLly6V++67T4YOHRrpU3S0P/74Q2644QbZs2ePTJw4UTp27OjZp9v69+8v+/fvN0VDTznlFBNe//Of/8iLL74oPXr0iOi5O8XGjRtNcdYKFSqYn4E6derIypUr5bPPPjOXAPrwww/NhaU3bNggAwYMkEqVKpnjdNuUKVPMLwitdt+hQ4dIv5Sot337dtMW+gtZv+dr1KhhCuX+9NNPXp9HtEX4jR8/Xl5++WXzf/0M0p8XxedUePTq1ctcn/Xuu+/22aeXImvQoEHwfy40JMW6CRMmuJo0aeKaNWuWZ1tBQYHrpptucjVv3ty1Y8eOiJ6fk82dO9d17rnnmvdf/3333Xde+5944gmz/aeffvJsO3LkiKt79+6u8847z3Xo0KEInLXzuL/X169f77X9vffeM+//66+/bu7fcsstrpYtW7q2bNniOWbfvn2ujh07urp16xb283aiESNGuM466yzXypUrPdvy8/NdPXv2NO/9/v37zTbaIrxWrVrlatasmatFixbmZyIvL8+zj8+p0NP3W997/fnwJ9g/Fwy3icjMmTMlIyNDunfv7tkWHx9vLm2Sl5dn2c2NwN1+++1y1113+bz3bvqX9KxZs0zXdbt27TzbK1eubP5a0yHSr7/+Osxn7TzHjh2TH3/8Udq3b296jYpyV69ftmyZ7N27VxYvXixdunQxf7G5paenS58+fWT9+vWeC0yj/PSzR4c69fveLSEhwQyDalvpX8q0RXjpFR0eeOABz/BnUXxOhcemTZvk+PHj0qRJkxKPCcXPRcyHJO2606EGnYcUFxfntc/9IbV69eoInZ2z6fuu8y5mzJghp59+us9+Hf7UodCivyzctL0UvwgCV7FiRTOc89RTT1l+6Lh/Sbvfa9ojtF544QV57bXXfLavWbPGBKi6devSFmE2cuRI87tC5+gVx+dUePz222/mtnHjxub2yJEjJqAWFYqfixMDqjFs165dOuRo5mAUV7VqVTOeuW3btoicm9PpL+bExES/baOs2qZ27drmlrYJnP7iLfpXV1FvvvmmuT3vvPNk586d5v+0R/joL2adszd58mQzR1Ln7ul77Z4QTFuE3ty5c2X69OlmPlLNmjV99vM5Fd6Q9M0335hFVjrHSP/Au/jii+WRRx6RU089NSSfUTEfkvRDSFWpUsVyf1JSkkmsCD5/AelkbaPtomib0NHVIvpPP3B0UqpOfiypPXRoQdEewfXggw/KokWLPH8dDx8+3Pz/4MGD5pa2CC0NQH//+9/NUE3Xrl0tj+FzKnyrb5UuKNGVa9WrV5cVK1bIpEmTzO3HH38ckp+LmA9JJ1vcp/uLD8Mh8m3j3kfbhIauzHniiSfMh42u5tFe1dIshKU9gkuX/usv6F9++UXeffddufbaa01YpS1CT9/jhx9+WKpVqyZ/+9vf/B53sn20ReB03qoOmenqTvcf2JdffrlZ4q+r3UaPHi1NmzY96eOUtS1iPiTpcJq/dKnb69evH+azQtG2OXr0qM8+d3ulpKSE/bycTkORDi1oMJowYYJnLJ/2CD+dgKq0F6Nly5amJ+mVV17xtAltETpajkSHOPVnQSfM6z+li3mULvfX4R5+LsLDvYikuCuuuML0dn/77beeifPBbIuYD0n16tUzydI9rly8G1Un5LnHMhFe7nDqHmcuyt1etE3w6If/448/blZ71qpVy8xHKvqXmb/28DcXAMELTBpcf/31V+nWrZvZRluEjg5zak+QDu1YufDCC83vj9dff93c53MqcrSW2O7du0PyGRXzIUk/dHTZs3ZnF+eeBa9FqhB+WtRTu7qtVhfSNsGlq0R0paFOUtUltm+88YbPh4n2ZOgkb6v2cG/Trm+Un64m1MK2WvFchw+Kh1jtzdB5LrRF6OlQW/Fq8+q5554z82Pefvtt0xZ8ToXn5+Kmm26SM844w1PMs+jPxebNm83ik1D8XMR8CQCllyD5888/5fPPP/dsKywsNNU5dezz6quvjuj5xSqtZqt/MeukPP3npl2putpHV5roygYEbuzYsSYg6TCOznmx+mtL32+thq7Hbd261bM9KyvLrP7RXie9tAzKT99j7dmeN2+eqelSlH4e6S8EHXqjLUJPg6q+x8X/paammv1at0qHd/icCk9PkdZIWrhwoaxdu9Zrn04J0FEfrVIfip8LLkvy329mfYM1jWrxL02rujx9yZIl5lo8WlQSoaXzLMaNG2d5WRItRa/jyfqXhBYF00nFWjNmzJgxnmEHlJ9+mFx55ZWe3iS9pILVh5Rek+r33383K910HsaQIUPMHxHukv/adlqQEoH54Ycf5NZbbzW93NqrlJaWZrb961//Mj0SOoFbL7lAW0SG/o7Q4qrFL0vC51Ro6e/j2267zfTe6eVGdEqAzhnTQKQlSt566y3zMxDsnwtCUpGkqd/MmlS1uqoGJX2DS5oshvCEJPcvcR160Ivf6l/SejHDO++8Uy655JKIna+TTJ061bKQZFH6y3natGnm//qXnP6s6AUjtWtb/+K+9957fSoRo/z0F7D+PGgldP0jTocS9PpfGp6Kls6gLewRkhSfU6Gn02J0Ir1+v2sg1Z8LHQnSjoxQ/VwQkgAAACwwJwkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMBCzF/gFoB19XMrWupfr2iuVYSHDx9uLp0Ri/744w85/fTTT1qFGUB046cZgCW9/pFewLOoXbt2mWuI6cVW9QrnkyZNkoSEBIklr776qvn366+/erYNHTpU+vTpE3PvBeB0hCQAlvQ6R9dee63Pdr1+mF7XUC+6umjRInNV+liyePFic22uoi688MKInQ+A0GFOEoAy0QtG9uvXz/z/p59+ivTpAEDIEJIAlFmVKlV8tulQ3N///ne5+OKLzVW3O3fuLM8884xkZ2d7HZeVlSWPPfaY6YHS47QXRq/QnZmZadlr89e//lXatm0rrVu3luuuu04+/fRTr2O0R0uvuK5DfzfffLN5TD2HRx991Gz/8ccffR53ypQpZp8OHSq9zvdHH30kgwYNkvbt20vz5s3loosukhEjRsjmzZs9X6dfs2LFCs//H3nkEc+cJL2fn5/vOfbo0aNmbteVV15pzqlDhw5mWG7lypU+c8D0a/X1P/HEE+b9aNmypbm6+cyZM72OLSgoMI/Zo0cP09On56rPvXDhwpO0GIDyYLgNQJktWLDA3Oovf7V161YZOHCgHD9+3Mxlqlevnvz222/ywQcfyL///W9zm56ebn7J63Ddtm3b5PrrrzfH6ddOnjxZvv32W/nyyy8lIyPDE2SefvppExjuuusu04Olz6vhZ+3atSZoFTVmzBg599xzTVD7888/5aqrrjKBatasWWZ7UTNmzJC0tDS57LLLzP2RI0eakHX55ZebYKShafny5fLFF1/Izz//LHPnzpWKFSvK888/b+Yj6cRt/f+pp55q+f4cOXJEbrzxRjNvS8OgBpm9e/ea90Ff94svvmjOr6g77rhDatWqZW71fXzvvffk4YcfNts6duxojnn22WfN+6I9eRoec3Jy5MMPP5Rhw4bJhAkTzIR6AEHkAoAiXn75ZVeTJk1ckyZNcu3bt8/zb8+ePa41a9a4Ro0a5TrrrLNcvXr1cuXn55uvufXWW11t27Z1bd682euxvvvuO/NYTz75pLm/evVqc/+NN97wOm7OnDmubt26uRYtWmTu//nnn67mzZu77rjjDldhYaHnOP3/gw8+aB5j1apVZtvSpUvN/csuu8xzPm69e/d2tW/f3nXs2DHPtvXr15vjR44cae5nZWW5mjVrZp6ruHvuucccq+ftNmDAALOtqBtuuMFsy8vLM/fHjRtn7o8dO9bruJ07d7o6dOjgateunSsnJ8fr/b755pu9XusPP/xgto8YMcKz7ZxzzjHvdVE7duxwde3a1fXKK6/4nD+AwDDcBsCS9uJccMEFnn86DNSzZ0+ZNm2a9O3bV95++22zmuvAgQOmF0iHfrQkgA6nuf81bdpUGjRoIPPmzTOPqb0i+jU6tPX555+br1XdunWTOXPmyKWXXmru6zCYTo7W3hYdrnM/nv7/6quvNsdo705ROpxVfHVZ7969TW9L0eEo9xCW7lPao6Rzq7R3pyj9uqSkJPP/gwcPlum9++qrr6Ry5cqmV6ioU045RW644QbJzc01Q4lF6RBaXFyc5767l057oNy0/IIOH7777rumN07VqVPHvL/a2wYguBhuA2DplltuMfNydOhpz549Zjhq3bp1cvfdd5u5P246Z6ewsFC+/vprE6ZKcuzYMRMSHn/8cRk1apTcf//9ZgitWbNm0qlTJxPA3LWHNm3aZG4feuihEh9v+/btXvdr1qzpc0z37t3lueeek88++8zMDdLz1OE3DSA6D8itUqVKJkjpcJ4Opelj6xwrd2jR96AstmzZYsKhBqXiGjdubG7dIaek809MTDS3es5uOiyo87d02E3/6XCfhlcNjsWHFAEEjpAEwNKZZ57pmQvj7u25/fbbTcDR0KTzZYr+Ete5NzrfpiTuXh6dHK2/1L/55hv57rvvzMTr1157Td58800ZO3asmRfkfsx//OMfctppp1k+ns5xKkoDV3HVqlWTK664wsx10p4onSe1c+dOrx4enf+jgVCLQer8J520ra9Vw5ueo871KSt/ocr92twhyN/5F6cT2OfPny9Lly41PVH63uk8J+3du+mmmzwTyQEEByEJQKnoL3UNMVo7SYtJapjQnpr69et7VnMVDVVu+ku9evXqphK1DpfpKi4dhtPVW/pPLVmyxAQVDUsaktyPmZKS4vOYu3fvltWrV5uemtLQYTXtPdLz0JVp2muk5+2mAUoDkj5/8Z4rneBdHtrDoxPS9T0p3pvkXsVXt27dMj2m9sRpT15qaqpZvaf/lD6P1q3Sid465BarVdCBUGBOEoBS07CjPUk6DPXUU0+ZXhkdJtLK3NorVHy5vfbE6OVL3njjDXNfez90pZf2fhSlPTgaotyX9NDeH+1Zef31181KsaJ0+Ewfs2jFa3/OO+88E6h0zpMGJQ1hGr7c3CUKmjRp4vV1OozoLhFQdGm/u0es6DBYcX/5y19MQCreC6U9cFOnTjWXd9GhzLLQnjBd1aZlFYrS16YrArVNStMbBaD06EkCUCbnn3++CTrvv/++/O1vfzMTuJ988kkzIVmHfLQEgAaOjRs3mjCkwco9NKcBRfe99NJLpgdEw9Hhw4fNUn0d9nLPddK5STr3SY/TuUq9evUywUbnDOkkca3BpEGqNDQ8aH0lfSyllw8pSudDjR492oQvnYukk8u1t2f69OmecKQTrd1q1Khhbl9++WUzWdyq90x7pbQauZYL0MfSuVr79u0z74c+lpYPsKo15Y9O0NZesU8++cQ8vpYv0NemwVPLFOj7X9bHBOAfIQlAmT3wwAMmrGjvkdbt0V/QGnQ0FGjvi9bu0d4NnSytNXzc84p0tdjEiRNND4v2MukwmNYf0rCkc5I0sLjp1+m8KA1j2hOlPTfaa6JDYhrSynKdNA1JWrRRg4aGvKIaNWpkHl9Djw4jKj1OX5Oev4Y0DSLuVXVaEFID4FtvvWXqIFmFJA0rWvtJH1eH83RSu86P0h43rROlhSDLQ+do6fnqCj2tC6V1pxo2bGhqQ+lcLwDBFad1AIL8mAAAAFGPAWwAAAALhCQAAAALhCQAAAALhCQAAAALhCQAAAALhCQAAAALhCQAAAALhCQAAAALhCQAAAALhCQAAAALhCQAAAALhCQAAAALhCQAAADx9f8BCIieX6QaoWwAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T15:31:36.084423Z",
     "start_time": "2025-05-21T15:31:36.078574Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "acea233bb27228df",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6728f075ee89bcf5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
